{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13395,
     "status": "ok",
     "timestamp": 1682129517346,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "LXO8RiR8Nznq",
    "outputId": "0732a1f8-3ec5-4512-dcd8-f9b055cde07c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/lib/python3.10/site-packages (4.29.1)\n",
      "Collecting tiktoken\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/5f/36/93115cf8bdb62284dd64c01893b208ac46e586cb6e01ad34bbe0784b90f4/tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4436,
     "status": "ok",
     "timestamp": 1682129521775,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "xMbzP_33BrcT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BejQteEwrCz8"
   },
   "source": [
    "# 0) Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "GPT model only has decoder block.\n",
    "While encoder block only generate the output with similar length as input,\n",
    "decoder is generative in nature.\n",
    "\n",
    "\n",
    "GPT-2 does not require the encoder part of the transformer architecture because the model uses a masked self-attention that can only look at prior tokens. The encoder is not needed because the model does not need to learn the representation of the input sequence.\n",
    "\n",
    "\n",
    "It produces estimates for the probability of the next word as outputs but it is auto-regressive as each token in the sentence has the context of the previous words. Thus GPT-2 works one token at a time.\n",
    "\n",
    "\n",
    "BERT, by contrast, is not auto-regressive. It uses the entire surrounding context all-at-once. GPT-2 the context vector is zero-initialized for the first word embedding.\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCwRiti8wjLY"
   },
   "source": [
    "# 1) Tokenization (Byte Pair Encoding)\n",
    "\n",
    "The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don’t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.\n",
    "\n",
    "Description: https://huggingface.co/course/chapter6/5?fw=pt\n",
    "\n",
    "Code: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRzhU2ZQCYdk"
   },
   "source": [
    "# 2) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3939,
     "status": "ok",
     "timestamp": 1682129525711,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "gFsw898QYE58",
    "outputId": "6cfabfe6-a59f-4903-a696-ad19c63db762",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "data_dir = os.path.join('data', 'tinyshakespeare')\n",
    "input_file_path = os.path.join(data_dir, 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    os.makedirs(data_dir)\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(data_dir, 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1682129525711,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "2iyOZwp6hfjF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class CustomConfig(GPTConfig):\n",
    "    # model\n",
    "    n_layer = 8\n",
    "    n_head = 8\n",
    "    n_embd = 256\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    dropout = 0.1\n",
    "    compile = True\n",
    "\n",
    "    # data\n",
    "    device = 'cuda'\n",
    "    num_workers = 0\n",
    "\n",
    "    # optimizer parameters\n",
    "    max_iters = 2e4\n",
    "    batch_size = 4\n",
    "    block_size = 64\n",
    "    learning_rate = 6e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    weight_decay = 1e-1\n",
    "    grad_norm_clip = 1.0\n",
    "\n",
    "# config\n",
    "vocab_size = len(train_ids)\n",
    "config = CustomConfig(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7606,
     "status": "ok",
     "timestamp": 1682129533312,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "J3LHo61xbFAb",
    "outputId": "e4337de5-a364-421f-86a2-cc460362650b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([4, 64])\n",
      "y: torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "# read data from .bin\n",
    "data_dir = os.path.join('data', 'tinyshakespeare')\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, split, block_size=128, device_type='cuda'):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.device_type = device_type\n",
    "        self.data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        # x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        # y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "        x = torch.from_numpy(self.data[idx : idx + self.block_size].astype(np.int64))\n",
    "        y = torch.from_numpy(self.data[idx + 1 : idx + 1 + self.block_size].astype(np.int64)) \n",
    "\n",
    "        if self.device_type == 'cuda':\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y = x.pin_memory().to('cuda', non_blocking=True), y.pin_memory().to('cuda', non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to('cpu'), y.to('cpu')\n",
    "        return x, y\n",
    "\n",
    "train_dataset = ShakespeareDataset('train', config.block_size, config.device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n",
    "test_dataset = ShakespeareDataset('test', config.block_size, config.device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n",
    "sample_data = next(iter(train_loader))\n",
    "x, y = sample_data\n",
    "print(\"x:\", x.size())\n",
    "print(\"y:\", y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKwwp8HewuOO"
   },
   "source": [
    "# 3) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code above defines a class CausalSelfAttention that implements the causal self-attention mechanism in the GPT model.##\n",
    "\n",
    "- The forward method is where the actual computation takes place. The method receives a tensor x of shape (batch_size, seq_len, emb_dim) as input.\n",
    "- It splits the input x into query, key, and value tensors for all heads and reshapes them accordingly. It then computes the attention score matrix using either the fast flash attention (torch.version ≥ 2.0)or the slower dot product method, depending on the pytorch version.\n",
    "- In the case of dot product attention, the attention score matrix is computed using matrix multiplication between the query and key tensors, followed by scaling by the square root of the key tensor’s dimension.\n",
    "- A mask is then applied to ensure that the attention is only applied to the left in the input sequence. In GPT, the masking is done using a triangular mask that blocks the model from attending to any word that comes after the current word in the sequence. To achieve this, we use torch.tril(torch.ones(n, n)) to create a lower-triangular matrix of ones. The tril function zeros out all elements above the diagonal of the matrix.\n",
    "- The resulting matrix is then normalized using the softmax function and multiplied by the value tensor to obtain the output. All these steps that we mentioned are actually the simple translation from the equation\n",
    "- Finally, the output is projected onto the same dimensionality as the input, using a residual connection and the output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4684,
     "status": "ok",
     "timestamp": 1682129537981,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "-gy9OcsmMXfD",
    "outputId": "3a9b94ce-0467-49bf-8154-b25a4f34ad39",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding Size: torch.Size([4, 64, 256])\n",
      "Block Output Size: torch.Size([4, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It's important in decoder block to have diagonal mask\n",
    "    It is also possible to use torch.nn.MultiheadAttention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.dropout = config.dropout\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(config.block_size, config.block_size)\n",
    "            ).view(1, 1, config.block_size, config.block_size))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # batch_size, seq_len, emb_dim\n",
    "        B, T, C = x.size() \n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # (b, seq_len, emb_dim) --> (b, seq_len, emb_dim * 3) --> (b, seq_len, emb_dim)\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            # (b, h, seq_len, d_k) matmul (b, h, d_k, seq_len) --> (b, h, seq_len, seq_len)\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            # diagonal mask\n",
    "            # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "            # (batch_size, h, seq_len, seq_len)\n",
    "            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "\n",
    "            # (b, h, seq_len, seq_len) matmul (b, h, seq_len, d_k) --> (b, h, seq_len, d_k)\n",
    "            y = att @ v \n",
    "\n",
    "        # (b, h, seq_len, d_k) --> (b, seq_len, h, d_k) --> (b, seq_len, d_model)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" GPT only contain decode block\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # (batch_size, seq_len, emb_dim)\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "### testing\n",
    "wte = nn.Embedding(config.vocab_size, config.n_embd).to(config.device)\n",
    "block = Block(config).to(config.device)\n",
    "\n",
    "tok_emb = wte(x)\n",
    "print('Token Embedding Size:', tok_emb.size())\n",
    "\n",
    "block_out = block(tok_emb)\n",
    "print('Block Output Size:', block_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The constructor (__init__) initializes the GPT model with the given configuration. The GPT model combined several components which are the embedding layer for word tokens wte, embedding layer for positional encoding wpe, decoder blocks Block and finally a layer normalization layer applied to the output of the transformer ln_f.\n",
    "- Meanwhile, the constructor initializes the weights of the GPT model using a special scaled initialization technique, as described in the GPT-2 paper. It also sets up an optimizer for training the model, with separate weight decay settings for different parts of the model.\n",
    "- The forward method computes the forward pass of the GPT model. It takes as input a tensor of word indices (idx) and a tensor of target indices (targets). The method first applies an embedding layer to the word indices and a positional encoding layer to the position indices. It then applies the transformer layers to the resulting tensor.\n",
    "- Next, it applies the language model head to the output of the transformer to obtain a probability distribution over the vocabulary.\n",
    "- Lastly, it computes the cross-entropy loss between the predicted distribution and the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47534,
     "status": "ok",
     "timestamp": 1682129585500,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "13LrwU6NR9bx",
    "outputId": "4fc3d13d-2d5f-4a40-8c3a-131b25bd50df",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 83.64M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2023-05-16 11:55:26,240] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  torch.Size([4, 64, 301966])\n",
      "loss:  tensor(12.6740, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        \n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "\n",
    "        # positional token, shape (1, t)\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        # (b, t, n_embd) -- > # (b, t, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        # -1 at output will be ignored\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b, t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "### testing\n",
    "wte = nn.Embedding(config.vocab_size, config.n_embd).to(config.device)\n",
    "model = GPT(config).to(config.device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# sample dataset from data loader\n",
    "logits, loss = model.forward(x, y)\n",
    "print('logits: ', logits.size())\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Generation ###\n",
    "GPT is an auto-regressive language model that takes in a conditioning sequence of indices and then generates new text one token at a time. The model generates each token based on the preceding tokens in the sequence.\n",
    "\n",
    "- The generate function is a method in the GPT class that generates new text based on a given input sequence. It takes in a conditioning sequence of indices idx of shape (batch size, sequence length). The function then completes the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "-It forward passes the model to get the logits for the index in the sequence. The logits represent the unnormalized probability distribution over the vocabulary of possible tokens.\n",
    "- Next, the function plucks the logits at the final step and scales them by a desired temperature. The temperature is used to control the randomness of the generated output. Higher temperatures lead to more diverse and random outputs, while lower temperatures lead to more conservative and predictable outputs.\n",
    "- Then, it applies softmax to convert the logits to normalized probabilities. The probabilities represent the likelihood of each token in the vocabulary to be the next token in the generated sequence.\n",
    "- Finally, the function either samples from the probability distribution using torch.multinomial(). It then appends the sampled index to the running sequence and continues the loop until max_new_tokens is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(301966, 256)\n",
       "    (wpe): Embedding(64, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=301966, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer ###\n",
    "After thoroughly exploring the GPT model and analyzing the provided source code, we are now equipped with the knowledge and understanding necessary to train the model using Shakespearean data. We can now confidently hit the “begin” button to initiate the training process and watch as the model learns to generate Shakespearean text.\n",
    "\n",
    "This code defines the Trainer class, which is responsible for training the GPT model.\n",
    "\n",
    "- It sets up a dictionary of callbacks, which will be triggered at various events during the training process.\n",
    "- The run method of the Trainer class sets up the optimizer and data loader, and then enters a loop that trains the model.\n",
    "- In each iteration of the loop, a batch of data is fetched, the model is forward-propagated, the loss is calculated, the gradients are backpropagated, and the parameters are updated using an optimizer.\n",
    "- The training progress is logged using the batch_end_callback, which is called every n iterations. The loop continues until a termination condition is met, which could be a maximum number of iterations specified in the configuration object.\n",
    "- A lightweight model is trained after 20000 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2184942,
     "status": "ok",
     "timestamp": 1682131770427,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "YqK1wjjLOhkW",
    "outputId": "1e5de401-4d95-41d0-8070-d6307eccece1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 83.64M\n",
      "iter_dt 0.00ms; iter 0: train loss 12.68121\n",
      "iter_dt 107.74ms; iter 500: train loss 5.84700\n",
      "iter_dt 109.06ms; iter 1000: train loss 5.15548\n",
      "iter_dt 109.84ms; iter 1500: train loss 4.64236\n",
      "iter_dt 107.30ms; iter 2000: train loss 5.25185\n",
      "iter_dt 109.86ms; iter 2500: train loss 4.20766\n",
      "iter_dt 126.89ms; iter 3000: train loss 4.47053\n",
      "iter_dt 114.59ms; iter 3500: train loss 4.71512\n",
      "iter_dt 106.33ms; iter 4000: train loss 4.37974\n",
      "iter_dt 107.54ms; iter 4500: train loss 4.69097\n",
      "iter_dt 106.41ms; iter 5000: train loss 4.37470\n",
      "iter_dt 107.34ms; iter 5500: train loss 4.54241\n",
      "iter_dt 106.78ms; iter 6000: train loss 4.19148\n",
      "iter_dt 111.34ms; iter 6500: train loss 4.45951\n",
      "iter_dt 109.48ms; iter 7000: train loss 3.87133\n",
      "iter_dt 111.91ms; iter 7500: train loss 4.31350\n",
      "iter_dt 112.46ms; iter 8000: train loss 3.86543\n",
      "iter_dt 110.74ms; iter 8500: train loss 3.99068\n",
      "iter_dt 105.17ms; iter 9000: train loss 4.26273\n",
      "iter_dt 120.13ms; iter 9500: train loss 3.57195\n",
      "iter_dt 110.80ms; iter 10000: train loss 3.98407\n",
      "iter_dt 111.08ms; iter 10500: train loss 3.57795\n",
      "iter_dt 128.56ms; iter 11000: train loss 3.66941\n",
      "iter_dt 115.42ms; iter 11500: train loss 3.56647\n",
      "iter_dt 107.27ms; iter 12000: train loss 4.05628\n",
      "iter_dt 105.81ms; iter 12500: train loss 3.52732\n",
      "iter_dt 104.54ms; iter 13000: train loss 3.95343\n",
      "iter_dt 106.83ms; iter 13500: train loss 3.74196\n",
      "iter_dt 104.48ms; iter 14000: train loss 3.87117\n",
      "iter_dt 108.47ms; iter 14500: train loss 4.16209\n",
      "iter_dt 110.68ms; iter 15000: train loss 4.34371\n",
      "iter_dt 105.77ms; iter 15500: train loss 4.08989\n",
      "iter_dt 104.52ms; iter 16000: train loss 3.42342\n",
      "iter_dt 106.71ms; iter 16500: train loss 3.71928\n",
      "iter_dt 110.23ms; iter 17000: train loss 4.44876\n",
      "iter_dt 105.24ms; iter 17500: train loss 3.60394\n",
      "iter_dt 90.35ms; iter 18000: train loss 4.48042\n",
      "iter_dt 109.53ms; iter 18500: train loss 3.95057\n",
      "iter_dt 110.67ms; iter 19000: train loss 3.63133\n",
      "iter_dt 108.87ms; iter 19500: train loss 3.75776\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, config, model, train_dataset):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "        self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
    "            shuffle=False,\n",
    "            # pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        data_iter = iter(train_loader)\n",
    "        while True:\n",
    "\n",
    "            # fetch the next batch (x, y) and re-init iterator if needed\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            x, y = batch\n",
    "\n",
    "            # forward the model\n",
    "            logits, self.loss = model(x, y)\n",
    "\n",
    "            # backprop and update the parameters\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            self.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            self.iter_num += 1\n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            # termination conditions\n",
    "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
    "                break\n",
    "\n",
    "model = GPT(config)\n",
    "trainer = Trainer(config, model, train_dataset)\n",
    "trainer = Trainer(config, model, train_dataset)\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 500 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use it to generate Shakespearean text based on any random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1682133243016,
     "user": {
      "displayName": "Kean Chan",
      "userId": "05792587367281359063"
     },
     "user_tz": -480
    },
    "id": "duAm60cLoT_f",
    "outputId": "b436e7f9-b4f3-4c5b-cbe5-430c4ba4d856",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord:\n",
      "Rise! My people, conquer the north!\n",
      "\n",
      "CAMILLO:\n",
      "I am too.\n",
      "\n",
      "FLORIZEL:\n",
      "I am a man: I am a king, and I,\n",
      "And I, that, to be a king,\n",
      "And, by my father\n"
     ]
    }
   ],
   "source": [
    "text = 'Lord:\\nRise! My people, conquer the north!'\n",
    "sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n",
    "sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n",
    "result = model.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n",
    "print(enc.decode(result.detach().cpu().tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'nanoGPT-module') else model  # Take care of distributed/parallel training\n",
    "torch.save(model.state_dict(), output_dir+\"nanoGPT.path\")\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
