{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69c72e1-5b56-4af9-8494-69c95b6188c4",
   "metadata": {},
   "source": [
    " implementing a simple GPT-like model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901214c0-fdbb-4f6a-875c-24769327bbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 32  # how many independent sequences will we process in parallel?\n",
    "BLOCK_SIZE = 64  # what is the maximum context length for predictions?\n",
    "MAX_ITER = 5000  # number of training iterations\n",
    "EVAL_INTER = 500\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_HEAD = 6\n",
    "NUM_EMBED = NUM_HEAD * 128\n",
    "NUM_LAYER = 6\n",
    "DROPOUT = 0.2\n",
    "\n",
    "\n",
    "def encode(text_seq: str, tokenizer: any) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Function to encode input text using a pre-trained tokenizer and vectorized lookups\n",
    "    \"\"\"\n",
    "    # tokenize the input text\n",
    "    tokens = tokenizer.tokenize(text_seq)\n",
    "    # convert the tokens to their corresponding ids\n",
    "    token_indices = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_indices = torch.tensor(token_indices, dtype=torch.long)\n",
    "    return token_indices\n",
    "\n",
    "\n",
    "def decode(enc_sec: torch.Tensor, tokenizer: any) -> str:\n",
    "    \"\"\"\n",
    "    Function to decode a sequence of token indices back to a string\n",
    "    \"\"\"\n",
    "    # convert the indices to a list\n",
    "    enc_sec = enc_sec.tolist()\n",
    "    # decode the indices to a string\n",
    "    text = tokenizer.decode(enc_sec)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_batch(data: list[str], block_size: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    This is a simple function to create batches of data.\n",
    "    GPUs allow for parallel processing we can feed multiple chunks at once\n",
    "    so that's why we would need batches - how many independant sequences\n",
    "    will we process in parallel.\n",
    "\n",
    "    Parameters:\n",
    "    data: list[str]: data to take batch from\n",
    "    block_size (int): size of the text that is proccessed at once\n",
    "    batch_size (int): number of sequences to process in parallel\n",
    "\n",
    "    Returns:\n",
    "    x, y: a tuple with token sequence and token target\n",
    "    \"\"\"\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # we stack batch_size rows of sentences\n",
    "    # so x and y are the matrices with rows_num=batch_size\n",
    "    # and col_num=block_size\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # y is x shifted one position right - because we predict\n",
    "    # word in y having all the previous words as context\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    data: list[str],\n",
    "    model: torch.nn.Module,\n",
    "    block_size: int,\n",
    "    batch_size: int,\n",
    "    eval_iters: int = 10,\n",
    "):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data=data, block_size=block_size, batch_size=batch_size)\n",
    "        logits, loss = model.forward(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_model_from_checkpoint(\n",
    "    model_class: torch.nn.Module,\n",
    "    path_to_checkpoint: str = \"checkpoints/state_dict_model.pt\",\n",
    "    **kwargs: dict,\n",
    ") -> torch.nn.Module:\n",
    "    try:\n",
    "        state_dict = torch.load(path_to_checkpoint)\n",
    "        print(\"Successfully loaded model from the checkpoint\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model from the checkpoint. {e}\")\n",
    "\n",
    "    model = model_class(**kwargs)\n",
    "    # load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model_to_chekpoint(\n",
    "    model: torch.nn.Module, path_to_checkpoint: str = \"checkpoints\", epoch: int = 0\n",
    "):\n",
    "    # check if path exists, otherwise create it\n",
    "    if not os.path.exists(path_to_checkpoint):\n",
    "        os.makedirs(path_to_checkpoint)\n",
    "\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
    "    checkpoint_name = \"checkpoint_epoch-\" + str(epoch) + \"_\" + dt_string + \".pt\"\n",
    "    full_path = os.path.join(path_to_checkpoint, checkpoint_name)\n",
    "    try:\n",
    "        torch.save(model.state_dict(), full_path)\n",
    "        print(\"Successfully saved the model to {}\".format(full_path))\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the model to checkpoint. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ea8580-6e22-4219-80c6-2366fbb68794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "##from utils import DEVICE\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of the self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, num_embed, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embed, head_size, bias=False)\n",
    "        # tril is a lower triangular matrix. it is not a parameter\n",
    "        # of the model, so we assign it to the module using register_buffer\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # let's also add dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        # Tril matrix (lower triagular matrix) is used to mask \n",
    "        # future positions (setting them toÂ -inf) so that the\n",
    "        # decoder \"learns\" to predict next words\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        # weighted aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v  # (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Heads of self-attention in parallel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, num_embed, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(\n",
    "                    head_size=head_size,\n",
    "                    num_embed=num_embed,\n",
    "                    block_size=block_size,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(num_embed, num_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output of the self-attention\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # apply the linear projection layer\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear layer followed by ReLu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # in the Attention is All You Need paper\n",
    "            # authors are using the size of the ffwd layer 2048\n",
    "            # and the output of the model is 512\n",
    "            # so we apply the same factor of 4\n",
    "            nn.Linear(num_embed, 4 * num_embed),\n",
    "            nn.ReLU(),\n",
    "            # apply the linear projection layer\n",
    "            nn.Linear(4 * num_embed, num_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This calss will group together MultiHead Attention and\n",
    "    FeedForward NN, so that we can copy it in Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, block_size, num_embed, dropout):\n",
    "        super().__init__()\n",
    "        head_size = num_embed // num_heads\n",
    "        self.sa = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            head_size=head_size,\n",
    "            num_embed=num_embed,\n",
    "            block_size=block_size,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.ffwd = FeedForward(num_embed=num_embed, dropout=dropout)\n",
    "        # add the layer normalization\n",
    "        self.ln1 = nn.LayerNorm(num_embed)\n",
    "        self.ln2 = nn.LayerNorm(num_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"x +\" is the skip (or residual) connection\n",
    "        # it helps with optimization\n",
    "        # also we apply layer normalization before self-attention\n",
    "        # and feed-forward (a reshufle from original paper)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        # a simple lookup table that stores embeddings of a fixed dictionary and size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # see more: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.vocab_size = kwargs.get(\"vocab_size\", 100)\n",
    "        self.num_embed = kwargs.get(\"num_embed\", 32)\n",
    "        self.block_size = kwargs.get(\"block_size\", 8)\n",
    "        self.num_heads = kwargs.get(\"num_heads\", 4)\n",
    "        self.num_layers = kwargs.get(\"num_layers\", 4)\n",
    "        self.dropout = kwargs.get(\"dropout\", 0.2)\n",
    "        # each token reads the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "        # each position from 0 to block_size-1 will get its embedding\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, self.num_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    num_heads=self.num_heads,\n",
    "                    block_size=self.block_size,\n",
    "                    num_embed=self.num_embed,\n",
    "                    dropout=self.dropout,\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # we add the layer norm before the Linear layer\n",
    "        self.ln_f = nn.LayerNorm(self.num_embed)\n",
    "        self.lm_head = nn.Linear(self.num_embed, self.vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are (B,T) tensor of integers\n",
    "        # the token_emb is (B, T, C), C = NUM_EMBED\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        # (T, C)\n",
    "        posit_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
    "\n",
    "        x = token_emb + posit_emb\n",
    "        # apply one head of self-attention\n",
    "        x = self.blocks(x)\n",
    "        # (B, T, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "        # compute the loss\n",
    "        if targets != None:\n",
    "            # cross_entropy accepts inputs in a (batch_size, num_classes)\n",
    "            # so we need to reformat our logits dimensions to\n",
    "            # (batch_size * time, dim_vocabulary), time = block_size\n",
    "            B, T, C = logits.shape\n",
    "            logits = torch.reshape(logits, (B * T, C))\n",
    "            targets = torch.reshape(targets, (B * T,))\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, block_size: int):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context too the  last block_size tokens\n",
    "            # because tokens don't communicate between blocks\n",
    "            idx_crop = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crop)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution with probabilities probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597aa42f-2f23-4ac4-8d40-16347c237e24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1239778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 89.48M parameters\n",
      "step          0 | train loss 10.7109 | val loss 10.7451\n",
      "step        500 | train loss 2.7981 | val loss 4.6958\n",
      "step       1000 | train loss 2.2503 | val loss 4.4527\n",
      "step       1500 | train loss 2.0824 | val loss 4.2747\n",
      "step       2000 | train loss 1.8708 | val loss 4.2316\n",
      "step       2500 | train loss 1.6076 | val loss 4.3298\n",
      "step       3000 | train loss 1.5019 | val loss 4.3197\n",
      "step       3500 | train loss 1.4121 | val loss 4.3275\n",
      "step       4000 | train loss 1.3343 | val loss 4.2657\n",
      "step       4500 | train loss 1.1640 | val loss 4.5715\n",
      "step       4999 | train loss 1.1217 | val loss 4.4900\n",
      "Successfully saved the model to checkpoints/checkpoint_epoch-4999_16.05.2023_22:26:12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 22:26:16.224465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] way. i'll do it you turn to drive home now. i'm difficult for me to come. i'm getting married. i'm getting married. i'm glad to have you had left. i'm glad to have you back. i'm glad to have you back. i'm glad you had friends. i'm glad you had friends. i'm glad you had friends. i'm glad you invited me. i'm glad you\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from model import Transformer\n",
    "from transformers import AutoTokenizer  # pip install transformers\n",
    "#from utils import (\n",
    "#    BATCH_SIZE,\n",
    "#    BLOCK_SIZE,\n",
    "#    DEVICE,\n",
    "#    DROPOUT,\n",
    "#    LEARNING_RATE,\n",
    "#    NUM_EMBED,\n",
    "#    NUM_HEAD,\n",
    "#    NUM_LAYER,\n",
    "#    MAX_ITER,\n",
    "#    EVAL_INTER,\n",
    "#    encode,\n",
    "#    decode,\n",
    "#    get_batch,\n",
    "#    save_model_to_chekpoint,\n",
    "#    estimate_loss,\n",
    "#)\n",
    "\n",
    "# load model from checkpoint\n",
    "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
    "\n",
    "# example to decode sequence\n",
    "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
    "# max_new_tokens=20)[0].tolist()\n",
    "# print(decode(vocab=vocab, enc_sec=enc_sec))\n",
    "\n",
    "# raw data\n",
    "path_do_data = \"data/english.txt\"\n",
    "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
    "# we use pretrained BERT tokenizer for performance improvements\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "# data_raw = data_raw[4000000:] # short dataset\n",
    "\n",
    "# train/val split\n",
    "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# train a new model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_embed=NUM_EMBED,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=NUM_HEAD,\n",
    "    num_layers=NUM_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "# load model to GPU if available\n",
    "m = model.to(DEVICE)\n",
    "# print the number of parameters in the model\n",
    "print(\n",
    "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
    ")\n",
    "# optimizer takes the model's parameters and the learning rate as input,\n",
    "# and updates the parameters during the training process in order to\n",
    "# minimize the loss function.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for step in range(MAX_ITER):\n",
    "\n",
    "    # every EVAL_INTER evaluate the loss on train and val sets\n",
    "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
    "        loss_train = estimate_loss(\n",
    "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        loss_val = estimate_loss(\n",
    "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backward() method on the loss variable calculates the gradients \n",
    "    # of the loss with respect to the model's parameters.\n",
    "    loss.backward()\n",
    "    # step() method on the optimizer updates the model's parameters \n",
    "    # using the calculated gradients, in order to minimize the loss.\n",
    "    optimizer.step()\n",
    "\n",
    "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoints\", epoch=step)\n",
    "\n",
    "# generate some output based on the context\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "print(\n",
    "    decode(\n",
    "        enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938f0a6-6d21-4c04-aec0-a533ae02c468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
