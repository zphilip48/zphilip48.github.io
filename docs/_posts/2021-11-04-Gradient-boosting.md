---
layout: post
title:  "Gradient boosting"
date:   2023-01-26
categories: LEARNING
tags: AI
---

## Gradient boosting <font color=red>performs gradient descent</font>

[Entropy of the Gaussian (gregorygundersen.com)](https://gregorygundersen.com/blog/2020/09/01/gaussian-entropy/)

**<font color=red>æ€»ç»“ï¼šå¯ä»¥è¯´ loss function (ä¸‹é¢åˆ—å‡º) æ˜¯ $\hat{y}$ çš„å‡½æ•°ï¼Œ é‚£ä¹ˆå®ƒçš„gradient boosting å…¶å®æ˜¯gradient descentï¼Œåªä¸è¿‡ç”±äºå…¶gradient å¯¼æ•° : $-2(\mathbf{y}-\hat{\mathbf{y}})$ of MSE and $-\operatorname{sign}(\mathbf{y}-\hat{\mathbf{y}})$ of MAE æ°å¥½æ˜¯ç›¸ååå‘çš„ $\mathbf{y}-\hat{\mathbf{y}}$  which is residual vector $\Delta_m(X)$, æ‰€ä»¥å¯¼è‡´boostingå…¬å¼çš„å½¢æ€.</font>**
$$
\begin{array}{cc}
\text { Gradient descent } & \text { Gradient boosting } \\
\mathbf{x}_t=\mathbf{x}_{t-1}-\eta \nabla f\left(\mathbf{x}_{t-1}\right) & \hat{\mathbf{y}}_m=\hat{\mathbf{y}}_{m-1}+\eta\left(-\nabla L\left(\mathbf{y}, \hat{\mathbf{y}}_{m-1}\right)\right)
\end{array}
$$
When *L* is the MSE loss function, *L*'s gradient is the residual vector and a gradient descent optimizer should chase that residual, which is exactly what the gradient boosting machine does as well. When *L* is the MAE loss function, *L*'s gradient is the sign vector, leading gradient descent and gradient boosting to step using the sign vector.

The implications of all of this fancy footwork is that **we can use a GBM to optimize any differentiable loss function by training our weak models on the negative of the loss function gradient** (with respect to the previous approximation). Understanding this derivation from the GBM recurrence relation to gradient descent update equation is much harder to see without the $\hat{\mathbf{y}}_m=F_m(X)$ substitution.

### **Boosting as gradient descent in prediction space**

Our goal is to show that training a GBMï¼ˆGradient Boosting Machineï¼‰ is performing gradient-descent minimization on some loss function between our true target, $\mathrm{y}$, and our approximation, $\hat{\mathbf{y}}_m=F_m(X)$. That means showing that adding weak models, $\Delta_m$, to our GBM additive model:
$$
F_m(X)=F_{m-1}(X)+\eta \Delta_m(X)
$$
**is performing gradient descent in some way**.   

It makes sense that nudging our approximation, $\hat{\mathbf{y}}$, closer and closer to the true target y would be performing gradient descent. For example, at each step, the residual $\mathbf{y}-\hat{\mathbf{y}}$ gets smaller. We must be minimizing some function related to the distance between the true target and our approximation. Let's revisit our golfer analogy and visualize the squared error between the approximation and the true value, $\left(y-F_m\right)^2$ :

<img src="/assets/Gradient%20boosting.assets/golf-MSE.png" alt="img" style="zoom: 25%;" />

**The key insight**
The key to unlocking the relationship for more than one observation is to see that the residual, **$y-\hat{y}$, is a direction vector**. It's not just the magnitude of the difference. Moreover, the vector points in the direction of a better approximation and, hence, a smaller loss between the true $y$ and $\hat{y}$ vectors. That <font color=red>suggests that the direction vector is also (the negative of) a loss function gradient</font>. **Chasing the direction vector in a GBM is chasing the (negative) gradient of a loss function via gradient descent.**

### **The MSE function gradient**

To uncover the loss function optimized by a GBM **whose $\Delta_m$ weak models are trained on the residual vector**, we just have to integrate the residual $\mathbf{y}-\hat{\mathbf{y}}$. It's actually easier, though, to go the other direction and compute the gradient of the MSE loss function to show that it is the residual vector. The MSE loss function computed from $N$ observations in matrix $X=\left[\mathbf{x}_1 \ldots \mathbf{x}_N\right]$ is:
$$
L\left(\mathbf{y}, F_M(X)\right)=\frac{1}{N} \sum_{i=1}^N\left(y_i-F_M\left(\mathrm{x}_i\right)\right)^2
$$
but let's substitute $\hat{y}$ for the model output, $F_M(X)$, to make the equation more clear:
$$
L(\mathbf{y}, \hat{\mathbf{y}})=\frac{1}{N} \sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2
$$
Also, since $N$ is a constant once we start boosting, and $f(x)$ and $c f(x)$ have the same $x$ minimum point, let's drop the $\frac{1}{N}$ constant:
$$
L(\mathbf{y}, \hat{\mathbf{y}})=\sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2
$$
Now, let's take the partial derivative of the loss function with respect to a specific $\hat{y}_j$ approximation:
$$
\begin{aligned}
\frac{\partial}{\partial \hat{y}_j} L(\mathbf{y}, \hat{\mathbf{y}}) & =\frac{\partial}{\partial \hat{y}_j} \sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2 \\
& =\frac{\partial}{\partial \hat{y}_j}\left(y_j-\hat{y}_j\right)^2 \\
& =2\left(y_j-\hat{y}_j\right) \frac{\partial}{\partial \hat{y}_j}\left(y_j-\hat{y}_j\right) \\
& =-2\left(y_j-\hat{y}_j\right)
\end{aligned}
$$
(We can remove the summation because the partial derivative of $L$ for $i \neq j$ is 0 .)
That means the gradient with respect to $\hat{y}$ is:
$$
\nabla_{\hat{\mathbf{y}}} L(\mathbf{y}, \hat{\mathbf{y}})=-2(\mathbf{y}-\hat{\mathbf{y}})
$$
Dropping the constant in front again leaves us with the <font color=red>**gradient being the same as the residual vector: $\mathbf{y}-\hat{\mathbf{y}}$**</font>. **So, chasing the residual vector in a GBM is chasing the gradient vector of the MSE $L_2$ loss function while performing gradient descent**.

### **The MAE(Mean Absolute Error) function gradient**

Let's see what happens with the MAE loss function:
$$
L(\mathbf{y}, \hat{\mathbf{y}})=\sum_{i=1}^N\left|y_i-\hat{y}_i\right|
$$
The partial derivative with respect to a specific approximation $\hat{y}_j$ is:
$$
\begin{aligned}
\frac{\partial}{\partial \hat{y}_j} L(\mathbf{y}, \hat{\mathbf{y}}) & =\frac{\partial}{\partial \hat{y}_j} \sum_{i=1}^N\left|y_i-\hat{y}_i\right| \\
& =\frac{\partial}{\partial \hat{y}_j}\left|y_j-\hat{y}_j\right| \\
& =\operatorname{sign}\left(y_j-\hat{y}_j\right) \frac{\partial}{\partial \hat{y}_j}\left(y_j-\hat{y}_j\right) \\
& =-\operatorname{sign}\left(y_j-\hat{y}_j\right)
\end{aligned}
$$
giving gradient:
$$
\nabla_{\hat{\mathbf{y}}} L(\mathbf{y}, \hat{\mathbf{y}})=-\operatorname{sign}(\mathbf{y}-\hat{\mathbf{y}})
$$
This shows that **chasing the sign vector in a GBM is chasing the gradient vector of the MAE $L_1$ loss function while performing gradient descent.**

### Function space is prediction space

æŠŠå‡½æ•°ç©ºé—´gradient descent è§£é‡Šä¸ºé¢„æµ‹å€¼çš„gradient descent ...å…¶å®å·®ä¸å¤š

Most GBM articles follow Friedman's notation (on page 4, equation for $g_m\left(\mathbf{x}_i\right)$ ) and describe the gradient as this scary-looking expression for the partial derivative with respect to our approximation of $y_i$ for observation $\mathbf{x}_i$ :
$$
\left[\frac{\partial L\left(y_i, F\left(\mathbf{x}_i\right)\right)}{\partial F\left(\mathbf{x}_i\right)}\right]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}
$$
Hmm... let's see if we can tease this apart. First, evaluate the expression according to the subscript, $F(\mathrm{x})=F_{m-1}(\mathrm{x})$
$$
\frac{\partial L\left(y_i, F_{m-1}\left(\mathrm{x}_i\right)\right)}{\partial F_{m-1}\left(\mathrm{x}_i\right)}
$$
Next, let's remove the $i$ index variable to look at the entire gradient, instead of a specific observation's partial derivative:
$$
\nabla_{F_{m-1}(X)} L\left(\mathbf{y}, F_{m-1}(X)\right)
$$
But, what does it mean to take the partial derivative with respect to a function, $F_{m-1}(X)$ ? Here is where we find it much easier to understand the gradient expression using $\hat{\mathbf{y}}_m$, rather than $F_m(X)$. Both are just vectors, but it's easier to see that when we use a simple variable name, $\hat{\mathbf{y}}_m$. Substituting, we get a gradient expression that references two vector variables $\mathbf{y}$ and $\hat{\mathbf{y}}_{m-1}$ :
$$
\nabla_{\hat{\mathbf{y}}_{m-1}} L\left(\mathbf{y}, \hat{\mathbf{y}}_{m-1}\right)
$$
Variable $\hat{\mathbf{y}}_{m-1}$ is a position in "function space," which just means a vector result of evaluating function $F_{m-1}(X)$. This is why GBMs perform "gradient descent in function space," but it's easier to think of it as "gradient descent in prediction space" where $\hat{\mathbf{y}}_{m-1}$ is our prediction.

æœ‰äº›è§£é‡Šä¸ºæ³°å‹’å±•å¼€å¼ï¼š

> åœ¨æ•°å­¦ä¸­ï¼Œæ³°å‹’å…¬å¼ (è‹±è¯­: Taylor's Formula) æ˜¯ä¸€ä¸ªç”¨å‡½æ•°åœ¨æŸç‚¹çš„ä¿¡æ¯æè¿°å…¶é™„è¿‘å–å€¼çš„å…¬å¼ã€‚è¿™ä¸ªå…¬å¼æ¥è‡ª äºå¾®ç§¯åˆ†çš„æ³°å‹’å®šç† (Taylor's theorem)ï¼Œæ³°å‹’å®šç†æè¿°äº†ä¸€ä¸ªå¯å¾®å‡½æ•°ï¼Œå¦‚æœå‡½æ•°è¶³ã¿Ÿå…‰æ»‘çš„è¯ï¼Œåœ¨å·²çŸ¥å‡½æ•°åœ¨ æŸä¸€ç‚¹çš„å„é˜¶å¯¼æ•°å€¼çš„æƒ…å†µä¹‹ä¸‹ï¼Œæ³°å‹’å…¬å¼å¯ä»¥ç”¨è¿™äº›å¯¼æ•°å€¼åšç³»æ•°æ„å»ºä¸€ä¸ªå¤šé¡¹å¼æ¥è¿‘ä¼¼å‡½æ•°åœ¨è¿™ä¸€ç‚¹çš„é‚»åŸŸä¸­ çš„å€¼ï¼Œè¿™ä¸ªç§°ä¸ºæ³°å‹’å¤šé¡¹å¼ (Taylor polynomial)ã€‚
> ç›¸å½“äºå‘Šè¯‰æˆ‘ä»¬å¯ç”±åˆ©ç”¨æ³°å‹’å†¬é¡¹å¼çš„æŸäº›æ¬¡é¡¹åšåŸå‡½æ•°çš„è¿‘ä¼¼ã€‚
> æ³°å‹’å®šç†:
> è®¾ $n$ æ˜¯ä¸€ä¸ªæ­£æ•´æ•°ã€‚å¦‚æœå®šä¹‰åœ¨ä¸€ä¸ªåŒ…å« $a$ çš„åŒºé—´ä¸Šçš„å‡½æ•° $f$ åœ¨ $a$ ç‚¹å¤„ $n+1$ æ¬¡å¯å¯¼ï¼Œé‚£ä¹ˆå¯¹äºè¿™ä¸ªåŒºé—´ä¸Šçš„ä»»æ„ x, éƒ½æœ‰:

$$
f(x)=f(a)+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{(2)}(a)}{2 !}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n !}(x-a)^n+R_n(x)
$$

> å…¶ä¸­çš„å¤šé¡¹å¼ç§°ä¸ºå‡½æ•°åœ¨ $\mathrm{a}$ å¤„çš„æ³°å‹’å±•å¼€å¼ï¼Œå‰©ä½™çš„ $R_n(x)$ æ˜¯æ³°å‹’å…¬å¼çš„ä½™é¡¹ï¼Œæ˜¯ $(x-a)^n$ çš„é«˜é˜¶æ— ç©·å°ã€‚
>
> <img src="/assets/Gradient%20boosting.assets/image-20230603235440862.png" alt="image-20230603235440862" style="zoom: 33%;" />
>
> åœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºæœ€ç»ˆçš„æœ€ä¼˜è§£ $\theta^*$ ï¼Œæ˜¯ç”±åˆå§‹å€¼ $\theta_0$ ç»è¿‡Tæ¬¡è¿­ä»£ä¹‹åå¾—åˆ° çš„ï¼Œè¿™é‡Œè®¾ $\theta_0=-\frac{\delta L(\theta)}{\delta \theta_0}$ ï¼Œåˆ™ $\theta^*$ ä¸º:

$$
\theta^*=\sum_{t=0}^T \alpha_t *\left[-\frac{\delta L(\theta)}{\delta \theta}\right]_{\theta=\theta_{t-1}}
$$

> å…¶ä¸­ï¼Œ $\left[-\frac{\delta L(\theta)}{\delta \theta}\right]_{\theta=\theta_{t-1}}$ è¡¨ç¤º $\theta$ åœ¨ $\theta_{t-1}$ å¤„æ³°å‹’å±•å¼€å¼çš„ä¸€é˜¶å¯¼æ•°ã€‚
> åœ¨å‡½æ•°ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å€Ÿé‰´æ¢¯åº¦ä¸‹é™çš„æ€æƒ³ï¼Œè¿›è¡Œæœ€ä¼˜å‡½æ•°çš„æœç´¢ã€‚å¯¹äºæ¨¡å‹çš„æŸå¤±å‡½æ•° $L(y, F(x))$ ï¼Œä¸ºäº†èƒ½å¤Ÿæ±‚è§£å‡ºæœ€ä¼˜çš„å‡½æ•° $F^*(x)$ ï¼Œé¦–å…ˆè®¾ç½®åˆå§‹å€¼ä¸º: $F_0(x)=f_0(x)$
> ä»¥å‡½æ•° $F(x)$ ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼Œä¸æ¢¯åº¦ä¸‹é™æ³•çš„æ›´æ–°è¿‡ç¨‹ä¸€è‡´ï¼Œå‡è®¾ç»è¿‡ Tæ¬¡è¿­ä»£å¾—åˆ°æœ€ä¼˜çš„å‡½æ•° $F^*(x)$ ä¸º:

$$
F^*(x)=\sum_{t=0}^T f_t(x)
$$

> å…¶ä¸­ï¼Œ $f_t(x)$ ä¸º:

$$
f_t(x)=-\alpha_t g_t(x)=-\alpha_t *\left[\frac{\delta L(y, F(x))}{\delta F(x)}\right]_{F(x)=F_{t-1}(x)}
$$

### How gradient boosting differs from gradient descent

Before finishing up, it's worth examining the differences between gradient descent and gradient boosting. To make things more concrete, let's consider applying gradient descent to train a neural network (NN). Training seeks to find the weights and biases, model parameters, that optimize the loss between the desired NN output, y , and the current output, $\hat{y}$. If we assume a squared error loss function, NN gradient descent training computes the next set of parameters by adding the residual vector, ,$y-\hat{y}$ to the current ![img](/assets/Gradient%20boosting.assets/eqn-8BB50605FF63759107F02187B2EE1A8D-depth000.00.svg+xml) (subtracting the squared error gradient).

**NN gradient descent è®¡ç®—ç»“æœä¼šä½œç”¨åœ¨å…¨éƒ¨å‚æ•°ä¸Šï¼Œ è€ŒGBMåªä¼šä½œç”¨åœ¨æ–°çš„weak modelä¸Š, è€çš„æ¨¡å‹ä¸Šçš„å‚æ•°æ˜¯å›ºå®šä¸å˜çš„ã€‚è€Œä¸”ç›´æ¥ä½œç”¨åœ¨è¾“å‡ºé¢„æµ‹å€¼ä¸Š**

In contrast, GBMs are meta-models consisting of multiple weak models whose output is added together to get an overall prediction. The optimization we're concerned with here occurs, **not on the parameters of the weak models** themselves but, instead, on the composite model prediction, $\hat{y}_m = F_m(x)$. GBM training occurs on two levels then, one to train the weak models and one on the overall composite model. It is the overall training of the composite model that performs gradient descent by adding the residual vector (assuming a squared error loss function) to get the improved model prediction. **Training a NN using gradient descent tweaks model parameters whereas training a GBM tweaks (boosts) the model output**.

Also, training a NN with gradient descent directly adds a direction vector to the current ![img](/assets/Gradient%20boosting.assets/eqn-8BB50605FF63759107F02187B2EE1A8D-depth000.00.svg+xml), whereas training a GBM adds a weak model's approximation of the direction vector to the current output, $\hat{y}$. Consequently, it's likely that a GBM's MSE and MAE will decrease monotonically during training but, **given the weak approximations of our $\Delta_m$, monotonicity is not guaranteed**. The GBM loss function could bounce around a bit on its way down.

One final note on training regression trees used for weak models. The interesting thing is that, regardless of the direction vector (negative gradient), regression trees can always get away with using the squared error to compute node split points; i.e., even when the overall GBM is minimizing the absolute error. The difference between optimizing MSE and MAE error for the GBM is that the weak models train on different direction vectors. How the regression trees compute splits is not a big issue since the stumps are really weak and give really noisy approximations anyway.

### General algorithm with regression tree weak models

This general algorithm assumes the use of regression trees and is more complex than the specific algorithms for $L_2$ and $L_1$ loss. We need to compute the gradient of the loss function, **instead of just using the residual or sign of the residual**, and we need to compute weights for regression tree leaves. Each leaf, $l$, has weight value, $w$, that minimizes the $\sum_{i \in l} L\left(y_i, F_{m-1}\left(\mathbf{x}_i\right)+w\right)$ for all $\mathbf{x}_i$ observations within that leaf.

![image-20230603211406691](/assets/Gradient%20boosting.assets/image-20230603211406691.png)

==è¿™é‡Œè²Œä¼¼æ²¡æœ‰è®²å¦‚ä½•ç”Ÿæˆregression tree to have the minimized MSE..  I think it could be the æœ€å°äºŒä¹˜å›å½’æ ‘ç”Ÿæˆç®—æ³• if the loss is MSE==

To see how this algorithm reduces to that for the $L_2$ loss function we have two steps to do. First, let $L\left(\mathbf{y}, \hat{\mathbf{y}}_{m-1}\right)=\left(\mathbf{y}-\hat{\mathbf{y}}_{m-1}\right)^2$, whose gradient gives the residual vector $\mathbf{r}_{m-1}=2\left(\mathbf{y}-\hat{\mathbf{y}}_{m-1}\right)$. **Second, show that leaf weight, $w$, is the mean of the residual of the observations in each leaf because the mean minimizes $\sum_{i \in l} L\left(y_i, F_{m-1}\left(\mathbf{x}_i\right)+w\right)$. That means minimizing**:
$$
\sum_{i \in l}\left(y_i-\left(F_{m-1}\left(\mathbf{x}_i\right)+w\right)\right)^2
$$
To find the minimal value of the function with respect to $w$, we take the partial derivative of that function with respect to $w$ and set to zero; then we solve for $w$. Here is the partial derivative:
$$
\frac{\partial}{\partial w} \sum_{i \in l}\left(y_i-\left(F_{m-1}\left(\mathbf{x}_i\right)+w\right)\right)^2=2 \sum_{i \in l}\left(y_i-\left(F_{m-1}\left(\mathbf{x}_i\right)+w\right)\right) \times \frac{\partial}{\partial w}\left(y_i-\left(F_{m-1}\left(\mathbf{x}_i\right)+w\right)\right)
$$
And since $\frac{\partial}{\partial w}\left(y_i-\left(F_{m-1}\left(\mathbf{x}_i\right)+w\right)\right)=1$, the last term drops off:
$$
2 \sum_{i \in l}\left(y_i-F_{m-1}\left(\mathbf{x}_i\right)-w\right)
$$
Now, set to 0 and solve for $w$ :
$$
\sum_{i \in l} 2 F_{m-1}\left(x_i\right)+2 w-2 y_i=0
$$
We can drop the constant by dividing both sides by 2 :
$$
\sum_{i \in l} F_{m-1}\left(\mathbf{x}_i\right)+w-y_i=0
$$
Then, pull out the $w$ term:
$$
\sum_{i \in l} F_{m-1}\left(\mathbf{x}_i\right)-y_i+\sum_{i \in l} w=0
$$
and move to the other side of the equation:
$$
\sum_{i \in l}\left(F_{m-1}\left(\mathbf{x}_i\right)-y_i\right)=-\sum_{i \in l} w
$$
We can simplify the $w$ summation to a multiplication:
$$
\sum_{i \in l}\left(F_{m-1}\left(\mathbf{x}_i\right)-y_i\right)=-n_l w \text { where } n_l \text { is number of obs. in } l
$$
Let's also flip the order of the elements within the summation to get the target variable first:
$$
\sum_{i \in l}\left(y_i-F_{m-1}\left(\mathbf{x}_i\right)\right)=n_l w
$$
Divide both sides of the equation by the number of observations in the leaf:
$$
w=\frac{1}{n_l} \sum_{i \in l}\left(y_i-F_{m-1}\left(\mathbf{x}_i\right)\right)
$$
**Finally, we see that leaf weights, $w$, should be the mean when the loss function is the mean squared error:**
$$
w=\operatorname{mean}\left(y_i-F_{m-1}\left(\mathbf{x}_i\right)\right)
$$
The mean is exactly what the leaves of the regression tree, trained on residuals, will predict.



### **Another version to have better understanding the relationship between $w$ (below is $\gamma$) and final model value $F_m(x)$**

refer to [All You Need to Know about Gradient Boosting Algorithm âˆ’ Part 1. Regression | by Tomonori Masui | Towards Data Science](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502)

Gradient Boosting Algorithm

1. Initialize model with a constant value:

$$
F_0(x)=\underset{\gamma}{\operatorname{argmin}} \sum_{i=1}^n L\left(y_i, \gamma\right)
$$

> if it is squared loss in our regression case:
> $$
> \begin{aligned}
> \frac{\partial}{\partial \gamma} \sum_{i=1}^n L & =\frac{\partial}{\partial \gamma} \sum_{i=1}^n\left(y_i-\gamma\right)^2 \\
> & =-2 \sum_{i=1}^n\left(y_i-\gamma\right) \\
> & =-2 \sum_{i=1}^n y_i+2 n \gamma
> \end{aligned}
> $$
>
> $$
> \begin{aligned}
> & -2 \sum_{i=1}^n y_i+2 n \gamma=0 \\
> & n \gamma=\sum_{i=1}^n y_i \\
> & \gamma=\frac{1}{n} \sum_{i=1}^n y_i=\bar{y}
> \end{aligned}
> $$
>
>  so we have the $F_0(x)=\stackrel{*}{\gamma}=\bar{y}$

2. for $m=1$ to $M$ :
   2-1. Compute residuals $r_{i m}=-\left[\frac{\partial L\left(y_i, F\left(x_i\right)\right)}{\partial F\left(x_i\right)}\right]_{F(x)=F_{m-1}(x)}$ for $i=1, \ldots, n$ , 

   > previous we already know $r_{i m}=-\left[\frac{\partial L\left(y_i, F\left(x_i\right)\right)}{\partial F\left(x_i\right)}\right]_{F(x)=F_{m-1}(x)}$  is equal to $\nabla_{\hat{\mathbf{y}}_{m-1}} L\left(\mathbf{y}, \hat{\mathbf{y}}_{m-1}\right)$

   2-2. Train regression tree with features $x$ against $r$ and create terminal node reasions $R_{j m}$ for $j=1, \ldots, J_m$
   2-3. Compute $\gamma_{j m}=\underset{\gamma}{\operatorname{argmin}} \sum_{x_i \in R_{\text {im }}} L\left(y_i, F_{m-1}\left(x_i\right)+\gamma\right)$ for $j=1, \ldots, J_m$

   > here we already know the $\gamma_{jm}$ will be the mean of the loss function is the mean squared error,  $w=\operatorname{mean}\left(y_i-F_{m-1}\left(\mathbf{x}_i\right)\right)$.  or another word $\gamma_{jm}$ is regular prediction values of regression trees that are the average of the target values (in our case, residuals) in each terminal node.

   2-4. Update the model:
   $$
   F_m(x)=F_{m-1}(x)+v \sum_{j=1}^{J_m} \gamma_{j m} 1\left(x \in R_{j m}\right)
   $$

   > In the final step, we are updating the prediction of the combined model $\mathrm{Fm}$. $\gamma_{j m} 1\left(x \in R_j m\right)$ means that **we pick the value $\gamma_{j m}$ if a given $x$ falls in a terminal node $R_{\mathrm{j}} m$.** As all the terminal nodes are exclusive, any given single $x$ falls into only a single terminal node and corresponding $\gamma_j m$ is added to the previous prediction $\mathrm F_{m-1}$ and it makes updated prediction $F m$.
   >
   > As mentioned in the previous section, $Î½$` is learning rate ranging between 0 and 1 which controls the degree of contribution of the additional tree prediction `*Î³*` to the combined prediction `*Fğ‘š*. A smaller learning rate reduces the effect of the additional tree prediction, **but it basically also reduces the chance of the model overfitting to the training data.**

### Regression Tree å›å½’æ ‘

äº‹å®ä¸Šï¼Œåˆ†ç±»ä¸å›å½’æ˜¯ä¸¤ä¸ªå¾ˆæ¥è¿‘çš„é—®é¢˜ï¼Œåˆ†ç±»çš„ç›®æ ‡æ˜¯æ ¹æ®å·²çŸ¥æ ·æœ¬çš„æŸäº›ç‰¹å¾ï¼Œåˆ¤æ–­ä¸€ä¸ªæ–°çš„æ ·æœ¬å±äºå“ªç§å·²çŸ¥çš„æ ·æœ¬ç±»ï¼Œå®ƒçš„ç»“æœæ˜¯ç¦»æ•£å€¼ã€‚è€Œå›å½’çš„ç»“æœæ˜¯è¿ç»­çš„å€¼ã€‚å½“ç„¶ï¼Œæœ¬è´¨æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯ç‰¹å¾ï¼ˆfeatureï¼‰åˆ°ç»“æœ/æ ‡ç­¾ï¼ˆlabelï¼‰ä¹‹é—´çš„æ˜ å°„ã€‚  

å¯¹äºå›å½’æ ‘ï¼Œä½ æ²¡æ³•å†ç”¨åˆ†ç±»æ ‘é‚£å¥—ä¿¡æ¯å¢ç›Šã€ä¿¡æ¯å¢ç›Šç‡ã€åŸºå°¼ç³»æ•°æ¥åˆ¤å®šæ ‘çš„èŠ‚ç‚¹åˆ†è£‚äº†ï¼Œä½ éœ€è¦é‡‡å–æ–°çš„æ–¹å¼è¯„ä¼°æ•ˆæœï¼ŒåŒ…æ‹¬é¢„æµ‹è¯¯å·®ï¼ˆå¸¸ç”¨çš„æœ‰å‡æ–¹è¯¯å·®ã€å¯¹æ•°è¯¯å·®ç­‰ï¼‰ã€‚è€Œä¸”èŠ‚ç‚¹ä¸å†æ˜¯ç±»åˆ«ï¼Œæ˜¯æ•°å€¼ï¼ˆé¢„æµ‹å€¼ï¼‰ï¼Œé‚£ä¹ˆæ€ä¹ˆç¡®å®šå‘¢ï¼Ÿæœ‰çš„æ˜¯èŠ‚ç‚¹å†…æ ·æœ¬å‡å€¼ï¼Œæœ‰çš„æ˜¯æœ€ä¼˜åŒ–ç®—å‡ºæ¥çš„æ¯”å¦‚Xgboostã€‚

CARTå›å½’æ ‘æ˜¯å‡è®¾æ ‘ä¸ºäºŒå‰æ ‘ï¼Œé€šè¿‡ä¸æ–­å°†ç‰¹å¾è¿›è¡Œåˆ†è£‚ã€‚æ¯”å¦‚å½“å‰æ ‘ç»“ç‚¹æ˜¯åŸºäºç¬¬jä¸ªç‰¹å¾å€¼è¿›è¡Œåˆ†è£‚çš„ï¼Œè®¾è¯¥ç‰¹å¾å€¼å°äºsçš„æ ·æœ¬åˆ’åˆ†ä¸ºå·¦å­æ ‘ï¼Œå¤§äºsçš„æ ·æœ¬åˆ’åˆ†ä¸ºå³å­æ ‘ã€‚ 

![img](data:image/png;base64,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)

 è€ŒCARTå›å½’æ ‘å®è´¨ä¸Šå°±æ˜¯åœ¨è¯¥ç‰¹å¾ç»´åº¦å¯¹æ ·æœ¬ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œè€Œè¿™ç§ç©ºé—´åˆ’åˆ†çš„ä¼˜åŒ–æ˜¯ä¸€ç§NPéš¾é—®é¢˜ï¼Œå› æ­¤ï¼Œåœ¨å†³ç­–æ ‘æ¨¡å‹ä¸­æ˜¯ä½¿ç”¨å¯å‘å¼æ–¹æ³•è§£å†³ã€‚å…¸å‹CARTå›å½’æ ‘äº§ç”Ÿçš„ç›®æ ‡å‡½æ•°ä¸ºï¼š 

![img](data:image/png;base64,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)

 å› æ­¤ï¼Œå½“æˆ‘ä»¬ä¸ºäº†æ±‚è§£æœ€ä¼˜çš„åˆ‡åˆ†ç‰¹å¾jå’Œæœ€ä¼˜çš„åˆ‡åˆ†ç‚¹sï¼Œå°±è½¬åŒ–ä¸ºæ±‚è§£è¿™ä¹ˆä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼š 

![img](data:image/png;base64,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)

 æ‰€ä»¥æˆ‘ä»¬åªè¦éå†æ‰€æœ‰ç‰¹å¾çš„çš„æ‰€æœ‰åˆ‡åˆ†ç‚¹ï¼Œå°±èƒ½æ‰¾åˆ°æœ€ä¼˜çš„åˆ‡åˆ†ç‰¹å¾å’Œåˆ‡åˆ†ç‚¹ã€‚æœ€ç»ˆå¾—åˆ°ä¸€æ£µå›å½’æ ‘ã€‚

ä¸€ä¸ªå›å½’æ ‘å¯¹åº”ç€è¾“å…¥ç©ºé—´ (å³ç‰¹å¾ç©ºé—´) çš„ä¸€ä¸ªåˆ’åˆ†ä»¥åŠåœ¨åˆ’åˆ†çš„å•å…ƒä¸Šçš„è¾“å‡ºå€¼ã€‚å‡è®¾å·²å°†è¾“ å…¥ç©ºé—´åˆ’åˆ†ä¸º $\mathrm{M}$ ä¸ªå•å…ƒ $R_1, R_2, \ldots, R_M$ ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªå•å…ƒ $R_m$ ä¸Šæœ‰ä¸€ä¸ªå›ºå®šçš„è¾“å‡ºå€¼ $c_m$ ï¼Œ äºæ˜¯å›å½’æ ‘æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º:
$$
f(x)=\sum_{m=1}^M c_m I\left(x \in R_m\right)
$$
å½“è¾“å…¥ç©ºé—´çš„åˆ’åˆ†ç¡®å®šæ—¶ï¼Œå¯ä»¥ç”¨å¹³æ–¹è¯¯å·® $\sum_{x_i \in R_m}\left(y_i-f\left(x_i\right)\right)^2$ æ¥è¡¨ç¤ºå›å½’æ ‘å¯¹äºè®­ç»ƒæ•°æ® çš„é¢„æµ‹è¯¯å·®ï¼Œç”¨å¹³æ–¹è¯¯å·®æœ€å°çš„å‡†åˆ™æ±‚è§£æ¯ä¸ªå•å…ƒä¸Šçš„æœ€ä¼˜è¾“å‡ºå€¼ã€‚æ˜“çŸ¥ï¼Œå•å…ƒ $R_m$ ä¸Šçš„ $c_m$ çš„ æœ€ä¼˜å€¼ $\hat{c}_m$ æ˜¯ $R_m$ ä¸Šçš„æ‰€æœ‰è¾“å…¥å®ä¾‹ $x_i$ å¯¹åº”çš„è¾“å‡º $y_i$ çš„å‡å€¼ï¼Œå³:
$$
\hat{c}_m=\operatorname{ave}\left(y_i \mid x_i \in R_m\right)
$$
<img src="/assets/Gradient%20boosting.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VtbWFfTG92ZQ==,size_16,color_FFFFFF,t_70.png" alt="img" style="zoom:50%;" />

#### é—®é¢˜ 1: æ€æ ·å¯¹è¾“å…¥ç©ºé—´è¿›è¡Œåˆ’åˆ†? å³å¦‚ä½•é€‰æ‹©åˆ’åˆ†ç‚¹?

CARTå›å½’æ ‘é‡‡ç”¨å¯å‘å¼çš„æ–¹æ³•å¯¹è¾“å…¥ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œé€‰æ‹©ç¬¬ $\mathrm{j}$ ä¸ªå˜é‡ $x^{(j)}$-- feature  å’Œå®ƒå–çš„å€¼Sï¼Œä½œä¸ºåˆ‡åˆ† å˜é‡ (splitting variable) å’Œåˆ‡åˆ†ç‚¹ (splitting point)ï¼Œå¹¶å®šä¹‰ä¸¤ä¸ªåŒºåŸŸ:
$$
R_1(j, s)=\left\{x \mid x^{(j)} \leq s\right\} \text { å’Œ } R_2(j, s)=\left\{x \mid x^{(j)}>s\right\}
$$
ç„¶åå¯»æ‰¾æœ€ä¼˜åˆ‡åˆ†å˜é‡ $\mathrm{j}$ å’Œæœ€ä¼˜åˆ‡åˆ†ç‚¹ $\mathrm{s}$ ã€‚å…·ä½“åœ°ï¼Œæ±‚è§£:
$$
\min _{j, s}\left[\min _{c_1} \sum_{x_i \in R_1(j, s)}\left(y_i-c_1\right)^2+\min _{c_2} \sum_{x_i \in R_2(j, s)}\left(y_i-c_2\right)^2\right]
$$
å¯¹å›ºå®šè¾“å…¥å˜é‡å¯ä»¥æ‰¾åˆ°æœ€ä¼˜åˆ‡åˆ†ç‚¹ $\mathrm{s}$ ã€‚

#### é—®é¢˜2: å¦‚ä½•å†³å®šæ ‘ä¸­å¶èŠ‚ç‚¹çš„è¾“å‡ºå€¼?

ç”¨é€‰å®šçš„æœ€ä¼˜åˆ‡åˆ†å˜é‡å’Œæœ€ä¼˜åˆ‡åˆ†ç‚¹åˆ’åˆ†åŒºåŸŸå¹¶å†³å®šç›¸åº”çš„è¾“å‡ºå€¼:
$$
\hat{c}_1=\operatorname{ave}\left(y_i \mid x_i \in R_1(j, s)\right) \text { å’Œ } \hat{c}_2=\operatorname{ave}\left(y_i \mid x_i \in R_2(j, s)\right)
$$
**éå†æ‰€æœ‰è¾“å…¥å˜é‡ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„åˆ‡åˆ†å˜é‡j, æ„æˆä¸€ä¸ªå¯¹ $(j, s)$** ã€‚ä¾æ­¤å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºä¸¤ä¸ªåŒº åŸŸã€‚æ¥ç€ï¼Œå¯¹æ¯ä¸ªåŒºåŸŸé‡å¤ä¸Šè¿°åˆ’åˆ†è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ä¸ºæ­¢ã€‚è¿™æ ·å°±ç”Ÿæˆä¸€é¢—å›å½’æ ‘ã€‚**è¿™æ · çš„å›å½’æ ‘é€šå¸¸ç§°ä¸ºæœ€å°äºŒä¹˜å›å½’æ ‘ (least squares regression tree)**ã€‚
å¦‚æœå·²å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸º $\mathrm{M}$ ä¸ªåŒºåŸŸ $R_1, R_2, \ldots, R_M$ ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªåŒºåŸŸ $R_m$ ä¸Šæœ‰ä¸€ä¸ªå›ºå®šçš„è¾“ å‡ºå€¼ $\hat{c}_m$ ï¼Œäºæ˜¯å›å½’æ ‘æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º:
$$
f(x)=\sum_{m=1}^M \hat{c}_m I\left(x \in R_m\right)
$$

#### ç®—æ³•æµç¨‹- (æœ€å°äºŒä¹˜å›å½’æ ‘ç”Ÿæˆç®—æ³•)

è¾“å…¥: è®­ç»ƒæ•°æ®é›† $D$;
è¾“å‡º: å›å½’æ ‘ $f(x)$.
åœ¨è®­ç»ƒæ•°æ®é›†æ‰€åœ¨çš„è¾“å…¥ç©ºé—´ä¸­, é€’å½’åœ°å°†æ¯ä¸ªåŒºåŸŸåˆ’åˆ†ä¸ºä¸¤ä¸ªå­åŒºåŸŸå¹¶å†³ å®šæ¯ä¸ªå­åŒºåŸŸä¸Šçš„è¾“å‡ºå€¼, æ„å»ºäºŒå‰å†³ç­–æ ‘:
ï¼ˆ1ï¼‰é€‰æ‹©æœ€ä¼˜åˆ‡åˆ†å˜é‡ $j$ ä¸åˆ‡åˆ†ç‚¹ $s$, æ±‚è§£
$$
\min _{j, s}\left[\min _{a_1} \sum_{x_i \in R_1(j, s)}\left(y_i-c_1\right)^2+\min _{c_2} \sum_{x_i \in R_2(j, s)}\left(y_i-c_2\right)^2\right]
$$
**éå†å˜é‡ $j$**, å¯¹å›ºå®šçš„åˆ‡åˆ†å˜é‡ $j$ æ‰«æåˆ‡åˆ†ç‚¹ $s$, é€‰æ‹©ä½¿ä¸Šå¼è¾¾åˆ°æœ€å° å€¼çš„å¯¹ $(j, s)$.
(2) ç”¨é€‰å®šçš„å¯¹ $(j, s)$ åˆ’åˆ†åŒºåŸŸå¹¶å†³å®šç›¸åº”çš„è¾“å‡ºå€¼:
$$
\begin{gathered}
R_1(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_2(j, s)=\left\{x \mid x^{(j)}>s\right\} \\
\hat{c}_m=\frac{1}{N_m} \sum_{x_i \in R_m(j, s)} y_i, \quad x \in R_m, m=1,2
\end{gathered}
$$
ï¼ˆ3ï¼‰ç»§ç»­å¯¹ä¸¤ä¸ªå­åŒºåŸŸè°ƒç”¨æ­¥ã—¶ (1), (2), ç›´è‡³æ»¡è¶³åœæ­¢æ¡ä»¶.
(4) å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸º $M$ ä¸ªåŒºåŸŸ $R_1, R_2, \cdots, R_M$, ç”Ÿæˆå†³ç­–æ ‘:
$$
f(x)=\sum_{m=1}^M \hat{c}_m I\left(x \in R_m\right)
$$

## Xgboost (extreme Boost)

XGBoost: A Scalable Tree Boosting System ï¼š https://arxiv.org/pdf/1603.02754v1.pdf

xgboostä¸gbdtæ¯”è¾ƒå¤§çš„ä¸åŒå°±æ˜¯ç›®æ ‡å‡½æ•°çš„å®šä¹‰ã€‚xgboostçš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹å›¾æ‰€ç¤º

![img](Machine_LearningNote.assets/format,png.png)

å…¶ä¸­

- çº¢è‰²ç®­å¤´æ‰€æŒ‡å‘çš„ $\mathrm{L}$ å³ä¸ºæŸå¤±å‡½æ•°ï¼ˆæ¯”å¦‚å¹³æ–¹æŸå¤±å‡½æ•°: $l\left(y_i, y^i\right)=\left(y_i-y^i\right)^2$ ï¼Œæˆ–logisticæŸå¤±å‡½ æ•°: $l\left(y_i, \hat{y}_i\right)=y_i \ln \left(1+e^{-\hat{y}_i}\right)+\left(1-y_i\right) \ln \left(1+e^{\hat{y}_i}\right)$,

- çº¢è‰²æ–¹æ¡†æ‰€æ¡†èµ·æ¥çš„æ˜¯æ­£åˆ™é¡¹ ï¼ˆregularization)ï¼ˆåŒ…æ‹¬L1æ­£åˆ™ã€L2æ­£åˆ™)

  $l_1$ æ­£åˆ™åŒ– ( $l_1$ Regularization)
  æ ¹æ®æƒé‡çš„ç»å¯¹å€¼çš„æ€»å’Œæ¥æƒ©ç½šæƒé‡ã€‚
  $$
  l_1: \Omega(w)=\|w\|_1=\sum_i\left|w_i\right|
  $$
  $l_2$ æ­£åˆ™åŒ– ( $l_2$ Regularization)
  æ ¹æ®æƒé‡çš„å¹³æ–¹å’Œæ¥æƒ©ç½šæƒé‡ã€‚
  $$
  l_2: \Omega(w)=\|w\|_2^2=\sum_i\left|w_i{ }^2\right|
  $$

- çº¢è‰²åœ†åœˆæ‰€åœˆèµ·æ¥çš„ä¸ºå¸¸æ•°é¡¹

- å¯¹äº $f(x)$ ï¼Œ xgboosåˆ©ç”¨æ³°å‹’å±•å¼€ä¸‰é¡¹ï¼Œåšä¸€ä¸ªè¿‘ä¼¼
  æˆ‘ä»¬å¯ä»¥å¾ˆæ¸…æ™°åœ°çœ‹åˆ°ï¼Œ**æœ€ç»ˆçš„ç›®æ ‡å‡½æ•°åªä¾èµ–äºæ¯ä¸ªæ•°æ®ç‚¹åœ¨è¯¯å·®å‡½æ•°ä¸Šçš„ä¸€é˜¶å¯¼æ•°å’ŒäºŒé˜¶å¯¼æ•°**ã€‚

xgboostçš„æ ¸å¿ƒç®—æ³•æ€æƒ³ç»§æ‰¿è‡ªGBMï¼ŒåŸºæœ¬å°±æ˜¯

1. ä¸æ–­åœ°æ·»åŠ æ ‘ï¼Œä¸æ–­åœ°è¿›è¡Œç‰¹å¾åˆ†è£‚æ¥ç”Ÿé•¿ä¸€æ£µæ ‘ï¼Œæ¯æ¬¡æ·»åŠ ä¸€ä¸ªæ ‘ï¼Œå…¶å®æ˜¯å­¦ä¹ ä¸€ä¸ªæ–°å‡½æ•°ï¼Œ å»æ‹Ÿåˆä¸Šæ¬¡é¢„æµ‹çš„æ®‹å·®ã€‚

$$
\begin{gathered}
\hat{y}=\phi\left(x_i\right)=\sum_{k=1}^K f_k\left(x_i\right) \\
\text { where } F=\left\{f(x)=w_{q(x)}\right\}\left(q: R^m \rightarrow T, w \in R^T\right)
\end{gathered}
$$

å…¶ä¸­ä¸€æ£µå›å½’æ ‘ã€‚

2. å½“æˆ‘ä»¬è®­ç»ƒå®Œæˆå¾—åˆ°kæ£µæ ‘ï¼Œæˆ‘ä»¬è¦é¢„æµ‹ä¸€ä¸ªæ ·æœ¬çš„åˆ†æ•°ï¼Œå…¶å®å°±æ˜¯æ ¹æ®è¿™ä¸ªæ ·æœ¬çš„ç‰¹å¾ï¼Œåœ¨æ¯ æ£µæ ‘ä¸­ä¼šè½åˆ°å¯¹åº”çš„ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹å°±å¯¹åº”ä¸€ä¸ªåˆ†æ•°
3. æœ€ååªéœ€è¦å°†æ¯æ£µæ ‘å¯¹åº”çš„åˆ†æ•°åŠ èµ·æ¥å°±æ˜¯è¯¥æ ·æœ¬çš„é¢„æµ‹å€¼ã€‚

æ˜¾ç„¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¦ä½¿å¾—æ ‘ç¾¤çš„é¢„æµ‹å€¼ $\hat{y}_i$ å°½é‡æ¥è¿‘çœŸå®å€¼ $y_i$ ï¼Œè€Œä¸”æœ‰å°½é‡å¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ‰€ä»¥ï¼Œä»æ•°å­¦è§’åº¦çœ‹è¿™æ˜¯ä¸€ä¸ªæ³›å‡½æœ€ä¼˜åŒ–é—®é¢˜ï¼Œ å·²çŸ¥è®­ç»ƒæ•°æ®é›† $T=\left\{\left(x_1, y_1\right),\left(x_2, y_2\right), \cdots,\left(x_n, y_n\right)\right\}$ ï¼ŒæŸå¤±å‡½æ•° $l\left(y_i, \widehat{y}_i\right)$ ï¼Œæ­£åˆ™åŒ–é¡¹ $\Omega\left(f_k\right)$ ï¼Œåˆ™æ•´ä½“ç›®æ ‡å‡½æ•°å¯è®°ä¸º :
$$
L(\phi)=\sum_i l\left(y_i, \hat{y}_i\right)+\sum_k \Omega\left(f_k\right)
$$
$\sum_k \Omega\left(f_k\right)$ è¡¨ç¤º $k$ æ£µæ ‘çš„å¤æ‚åº¦
å…¶ä¸­ :
$>\mathcal{L}(\phi)$ æ˜¯çº¿æ€§ç©ºé—´ä¸Šçš„è¡¨è¾¾ã€‚
$>i$ æ˜¯ç¬¬ $i$ ä¸ªæ ·æœ¬ï¼Œ $k$ æ˜¯ç¬¬ $k$ æ£µæ ‘ã€‚
$>\hat{y}_i$ æ˜¯ç¬¬ $i$ ä¸ªæ ·æœ¬ $x_i$ çš„é¢„æµ‹å€¼
$$
\widehat{y}_i=\sum_{k=1}^K f_k\left(x_i\right)
$$
å¦‚ä½ æ‰€è§ï¼Œè¿™ä¸ªç›®æ ‡å‡½æ•°åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šæŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ã€‚ä¸”æŸå¤±å‡½æ•°æ­ç¤ºè®­ç»ƒè¯¯å·®ï¼ˆå³é¢„æµ‹åˆ†æ•° å’ŒçœŸå®åˆ†æ•°çš„å·®è·)ï¼Œæ­£åˆ™åŒ–å®šä¹‰å¤æ‚åº¦ã€‚ æ‚åº¦è¶Šä½ï¼Œæ³›åŒ–èƒ½åŠ›è¶Šå¼ºï¼Œå…¶è¡¨è¾¾å¼ä¸º
$$
\Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^2
$$
Tè¡¨ç¤ºå¶å­èŠ‚ç‚¹çš„ä¸ªæ•°ï¼Œwè¡¨ç¤ºå¶å­èŠ‚ç‚¹çš„åˆ†æ•°ã€‚ç›´è§‚ä¸Šçœ‹ï¼Œç›®æ ‡è¦æ±‚é¢„æµ‹è¯¯å·®å°½é‡å°ï¼Œä¸”å¶å­èŠ‚ç‚¹ $T$ å°½é‡å°‘ï¼ˆYæ§åˆ¶å¶å­ç»“ç‚¹çš„ä¸ªæ•°ï¼‰ï¼ŒèŠ‚ç‚¹æ•°å€¼wå°½é‡ä¸æç«¯ï¼ˆ $\lambda$ æ§åˆ¶å¶å­èŠ‚ç‚¹çš„åˆ†æ•°ä¸ä¼šè¿‡å¤§ï¼‰ï¼Œé˜²æ­¢ è¿‡æ‹Ÿåˆã€‚

#### **æ¨¡å‹å­¦ä¹ ä¸è®­ç»ƒè¯¯å·®**

å…·ä½“æ¥è¯´ï¼Œç›®æ ‡å‡½æ•°ç¬¬ä¸€éƒ¨åˆ†ä¸­çš„ $i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªæ ·æœ¬ï¼Œ $l\left(\hat{y}_i-y_i\right)$ è¡¨ç¤ºç¬¬ $i$ ä¸ªæ ·æœ¬çš„é¢„æµ‹è¯¯å·®ï¼Œæˆ‘ä»¬çš„ ç›®æ ‡å½“ç„¶æ˜¯è¯¯å·®è¶Šå°è¶Šå¥½ã€‚
ç±»ä¼¼ä¹‹å‰GBDTçš„å¥—è·¯ï¼Œ xgboostä¹Ÿæ˜¯éœ€è¦å°†å¤šæ£µæ ‘çš„å¾—åˆ†ä¾½åŠ å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹å¾—åˆ†ï¼ˆæ¯ä¸€æ¬¡è¿­ä»£ï¼Œéƒ½ åœ¨ç°æœ‰æ ‘çš„åŸºç¡€ä¸Šï¼Œå¢åŠ ä¸€æ£µæ ‘å»æ‹Ÿåˆå‰é¢æ ‘çš„é¢„æµ‹ç»“æœä¸çœŸå®å€¼ä¹‹é—´çš„æ®‹å·®)ã€‚

- Start from constant prediction, add a new function each time

$$
\begin{aligned}
\hat{y}_i^{(0)} & =0 \\
\hat{y}_i^{(1)} & =f_1\left(x_i\right)=\hat{y}_i^{(0)}+f_1\left(x_i\right) \\
\hat{y}_i^{(2)} & =f_1\left(x_i\right)+f_2\left(x_i\right)=\hat{y}_i^{(1)}+f_2\left(x_i\right) \\
& \cdots \\
\hat{y}_i^{(t)} & =\sum_{k=1}^t f_k\left(x_i\right)=\hat{y}_i^{(t-1)} +f_t\left(x_i\right) \\
&\hat{y}_i^{(t)} \text {; Model at training round } \mathbf{t} \\
&\hat{y}_i^{(t-1)} \text{; Keep functions added in previous round } \\
&f_t\left(x_i\right) \text{;New function}
\end{aligned}
$$

ä½†ï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©æ¯ä¸€è½®åŠ å…¥ä»€ä¹ˆ $f$ å‘¢? ç­”æ¡ˆæ˜¯éå¸¸ç›´æ¥çš„ï¼Œé€‰å–ä¸€ä¸ª $f$ æ¥ä½¿å¾—æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°å°½é‡ æœ€å¤§åœ°é™ä½ã€‚
$$
\begin{aligned}
& O b j^{(t)}=\sum_{i=1}^n l\left(y_i, \hat{y}_i^{(t)}\right)+\sum_{i=1}^t \Omega\left(f_i\right) \\
&=\sum_{i=1}^n l\left(y_i, \hat{y}_i^{(t-1)}+f_t\left(x_i\right)\right)+\Omega\left(f_t\right)+\text { constant } \\
& \text { Goal: find } f_t \text { to minimize this }
\end{aligned}
$$
å†å¼ºè°ƒä¸€ä¸‹ï¼Œè€ƒè™‘åˆ°ç¬¬è½®çš„æ¨¡å‹é¢„æµ‹å€¼ $\hat{y}_i^{(t)}=$ å‰ $\mathrm{t}-1$ è½®çš„æ¨¡å‹é¢„æµ‹ $\hat{y}_i^{(t-1)}+f_t\left(x_i\right)$ ï¼Œå› æ­¤è¯¯å·®å‡½æ•°è®° ä¸º: $l\left(y_i, \hat{y}_i^{(t-1)}+f_t\left(x_i\right)\right)$ ï¼Œåé¢ä¸€é¡¹ä¸ºæ­£åˆ™åŒ–é¡¹ã€‚**é™¤äº†regularization å…¶ä»–å’ŒGBMæ˜¯ä¸€è‡´çš„**

#### ç¬¬ä¸€æ­¥ï¼šäºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå»é™¤å¸¸æ•°é¡¹

<img src="/assets/Gradient%20boosting.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZfSlVMWV92,size_16,color_FFFFFF,t_70-1685933461156-70.png" alt="img" style="zoom:50%;" />

$f(x)$ åœ¨ $x$ å¤„è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€å¾—åˆ° :
$$
f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^2
$$
**å¯¹ $l(y_i, \hat{y}^{t})$åœ¨ $\hat{y}_i^{(t-1)}$ å¤„è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€å¾—åˆ°** :
$>\widehat{y}_i^{(t-1)}$ æ˜¯å·²çŸ¥çš„, $\Delta = f_t\left(x_i\right)$
$$
l\left(y_i, \widehat{y}_i^{(t-1)}+f_t(x_i)\right) \approx l\left(y_i, \widehat{y}_i^{(t-1)}\right)+l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)\left(\widehat{y}_i^{(t-1)}+f_t(x_i)-\widehat{y}_i^{(t-1)}\right)+\frac{l^{\prime \prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)}{2}\left(\widehat{y}_i^{(t-1)}+f_t(x_i)-\widehat{y}_i^{(t-1)}\right)^2 \\

l\left(y_i, \widehat{y}_i^{(t-1)}+f_t(x_i)\right) \approx l\left(y_i, \widehat{y}_i^{(t-1)}\right)+l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)f_t(x_i)+\frac{l^{\prime \prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)}{2}(f_t(x_i))^2
$$
è®°ä¸€é˜¶å¯¼ä¸º $g_i=l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)$ ï¼ŒäºŒé˜¶å¯¼ä¸º $h_i=l^{\prime \prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)$.  

**Recall GBM if the lost function is MSE we have  $\nabla_{\hat{\mathbf{y}}} L(\mathbf{y}, \hat{\mathbf{y}})=-2(\mathbf{y}-\hat{\mathbf{y}})$ and $g_i=l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)= - 2(y_i-\hat{y}^{(t-1)}_i)$ and $h_i=l^{\prime \prime}\left(y_i, \widehat{y}_i^{(t-1)}\right) = 2 $.**

å¾—åˆ° $l\left(y_i, \widehat{y}_i^{(t-1)}+f_t\left(x_i\right)\right)$ çš„äºŒé˜¶æ³°å‹’å±•å¼€ :
$$
l\left(y_i, \widehat{y}_i^{(t-1)}+f_t\left(x_i\right)\right) \approx l\left(y_i, \widehat{y}_i^{(t-1)}\right)+g_i f_t\left(x_i\right)+\frac{h_i}{2} f_t^2\left(x_i\right)
$$
å¸¦å…¥ç›®æ ‡å‡½æ•°å¯å¾— :
$$
\mathcal{L}^{(t)}=\sum_{i=1}^n\left[l\left(y_i, \widehat{y}_i^{(t-1)}\right)+g_i f_t\left(x_i\right)+\frac{1}{2} h_i f_t^2\left(x_i\right)\right]+\sum_k \Omega\left(f_k\right)
$$

#### ç¬¬äºŒæ­¥ï¼šæ­£åˆ™åŒ–é¡¹å±•å¼€ï¼Œå»é™¤å¸¸æ•°é¡¹

æ¥ä¸‹æ¥ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬çš„ç¬¬ $\mathrm{t}$ é¢—å›å½’æ ‘æ˜¯æ ¹æ®å‰é¢çš„t-1é¢—å›å½’æ ‘çš„æ®‹å·®å¾—æ¥çš„ï¼Œç›¸å½“äºt-1 é¢—æ ‘çš„å€¼ $\hat{y}_i^{(t-1)}$ æ˜¯å·²çŸ¥çš„ã€‚æ¢å¥è¯è¯´ï¼Œ $l\left(y_i, \hat{y}_i^{(t-1)}\right)$ å¯¹ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–ä¸å½±å“ï¼Œå¯ä»¥ç›´æ¥å»æ‰ï¼Œä¸”å¸¸æ•°é¡¹ ä¹Ÿå¯ä»¥ç§»é™¤ï¼Œä»è€Œå¾—åˆ°å¦‚ä¸‹ä¸€ä¸ªæ¯”è¾ƒç»Ÿä¸€çš„ç›®æ ‡å‡½æ•°ã€‚

å°†æ­£åˆ™é¡¹è¿›è¡Œæ‹†åˆ†å¾—
$$
\sum_k \Omega\left(f_k\right)=\sum_{k=1}^t \Omega\left(f_k\right)=\Omega\left(f_t\right)+\sum_{k=1}^{t-1} \Omega\left(f_k\right)=\Omega\left(f_t\right)+\text { å¸¸æ•° }
$$
å› ä¸º $t-1$ æ£µæ•°çš„ç»“æ„å·²ç»ç¡®å®šï¼Œæ‰€ä»¥$\sum_{k=1}^{t-1} \Omega\left(f_k\right)=\text { å¸¸æ•° }$ å³ç›®æ ‡å‡½æ•°å¯è®°ä¸º

- Objective, with constants removed

$$
\sum_{i=1}^n\left[g_i f_t\left(x_i\right)+\frac{1}{2} h_i f_t^2\left(x_i\right)\right]+\Omega\left(f_t\right)
$$

- where $g_i=\partial_{\hat{y}^{(t-1)}} l\left(y_i, \hat{y}^{(t-1)}\right), \quad h_i=\partial_{\hat{y}^{(t-1)}}^2 l\left(y_i, \hat{y}^{(t-1)}\right)$

è¿™æ—¶ï¼Œç›®æ ‡å‡½æ•°åªä¾èµ–äºæ¯ä¸ªæ•°æ®ç‚¹åœ¨è¯¯å·®å‡½æ•°ä¸Šçš„ä¸€é˜¶å¯¼æ•° $g$ å’ŒäºŒé˜¶å¯¼æ•° $h$

because of $\Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^2$
$$
\begin{aligned}
O b j^{(t)} & \simeq \sum_{i=1}^n\left[g_i f_t\left(x_i\right)+\frac{1}{2} h_i f_t^2\left(x_i\right)\right]+\Omega\left(f_t\right) \\
& =\sum_{i=1}^n\left[g_i w_{q\left(x_i\right)}+\frac{1}{2} h_i w_{q\left(x_i\right)}^2\right]+\gamma T+\lambda \frac{1}{2} \sum_{j=1}^T w_j^2 \\
& =\sum_{j=1}^T\left[\left(\sum_{i \in I_j} g_i\right) w_j+\frac{1}{2}\left(\sum_{i \in I_j} h_i+\lambda\right) w_j^2\right]+\gamma T
\end{aligned}
$$
å°†å±äºç¬¬ $j$ ä¸ªå¶å­ç»“ç‚¹çš„æ‰€æœ‰æ ·æœ¬ $x_i$ ï¼Œåˆ’å…¥åˆ°ä¸€ä¸ªå¶å­ç»“ç‚¹æ ·æœ¬é›†åˆä¸­ï¼Œæ•°å­¦æè¿°å¦‚ä¸‹: $I_j=\left\{i \mid q\left(x_i\right)=j\right\}$  --- the instance set in leaf $j$ as $I_j$

#### ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶ä¸€æ¬¡é¡¹ç³»æ•°ã€äºŒæ¬¡é¡¹ç³»æ•°

- ç”¨å¶å­èŠ‚ç‚¹é›†åˆä»¥åŠå¶å­èŠ‚ç‚¹å¾—åˆ†è¡¨ç¤º
- æ¯ä¸ªæ ·æœ¬éƒ½è½åœ¨ä¸€ä¸ªå¶å­èŠ‚ç‚¹ä¸Š
- q(x)è¡¨ç¤ºæ ·æœ¬xåœ¨æŸä¸ªå¶å­èŠ‚ç‚¹ä¸Šï¼Œwq(x)æ˜¯è¯¥èŠ‚ç‚¹çš„æ‰“åˆ†ï¼Œå³è¯¥æ ·æœ¬çš„æ¨¡å‹é¢„æµ‹å€¼

<img src="/assets/Gradient%20boosting.assets/image-20230604204030621.png" alt="image-20230604204030621" style="zoom:50%;" />

<img src="/assets/Gradient%20boosting.assets/image-20230604203954383.png" alt="image-20230604203954383" style="zoom:50%;" />

<img src="/assets/Gradient%20boosting.assets/image-20230604204045839.png" alt="image-20230604204045839" style="zoom:50%;" />

<img src="/assets/Gradient%20boosting.assets/image-20230604204125881.png" alt="image-20230604204125881" style="zoom:50%;" />

#### **XGBoostç›®æ ‡å‡½æ•°è§£**

å·²çŸ¥XGBoostçš„ç›®æ ‡å‡½æ•° :
$$
\mathcal{L}^{(t)}=\sum_{j=1}^T\left[G_j w_j+\frac{1}{2}\left(H_j+\lambda\right) w_j^2\right]+\gamma T
$$
åˆ™æ¯ä¸ªå¶å­ç»“ç‚¹ $j$ çš„ç›®æ ‡å‡½æ•°æ˜¯ :
$$
f\left(w_j\right)=G_j w_j+\frac{1}{2}\left(H_j+\lambda\right) w_j{ }^2
$$
å…¶æ˜¯ä¸€ä¸ª $w_j$ çš„ä¸€å…ƒäºŒæ¬¡å‡½æ•°ã€‚ $\left(H_j+\lambda\right)>0$ ï¼Œåˆ™ $f\left(w_j\right)$ åœ¨ $w_j=-\frac{G_j}{H_j+\lambda}$ å¤„å–å¾—æœ€å°å€¼ï¼Œæœ€å°å€¼ä¸º $=-\frac{1}{2} \frac{G_j{ }^2}{H_j+\lambda}$

XGBoostç›®æ ‡å‡½æ•°çš„å„ä¸ªå¶å­ç»“ç‚¹çš„ç›®æ ‡å¼å­æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚å³æ¯ä¸ªå¶å­ç»“ç‚¹çš„å¼å­éƒ½è¾¾åˆ°æœ€å€¼ç‚¹ï¼Œæ•´ä¸ªç›®æ ‡å‡½æ•°ä¹Ÿè¾¾åˆ°æœ€å€¼ç‚¹ã€‚
åˆ™æ¯ä¸ªå¶å­ç»“ç‚¹çš„æƒé‡ $w_j^*$ åŠæ­¤æ—¶è¾¾åˆ°æœ€ä¼˜çš„ $O b j$ ç›®æ ‡å€¼ :
$$
w_j^*=-\frac{G_j}{H_j+\lambda} \quad O b j=-\frac{1}{2} \sum_{j=1}^T \frac{G_j{ }^2}{H_j+\lambda}+\gamma T
$$
$>$ ç›®æ ‡å€¼ $O b j$ æœ€å°ï¼Œåˆ™æ ‘ç»“æ„æœ€å¥½ï¼Œæ­¤æ—¶å³æ˜¯ç›®æ ‡å‡½æ•°çš„æœ€ä¼˜è§£ã€‚

#### æ‰“åˆ†å‡½æ•°ï¼ˆç»“æ„åˆ†æ•°structure score -- Obj function value)

<img src="/assets/Gradient%20boosting.assets/20160421110535150.png" alt="img" style="zoom:80%;" />

#### åˆ†è£‚èŠ‚ç‚¹

##### (1) æšä¸¾æ‰€æœ‰ä¸åŒæ ‘ç»“æ„çš„è´ªå¿ƒæ³• Greedy Learning   

==[zphilip48: æœ€å°äºŒä¹˜å›å½’æ ‘ ç®—æ³•ç±»ä¼¼ï¼ŒåŒºåˆ«åœ¨äºé€‰æ‹©(å¢ç›Š)æ¡ä»¶å˜åŒ–]==

ç°åœ¨çš„æƒ…å†µæ˜¯åªè¦çŸ¥é“æ ‘çš„ç»“æ„ï¼Œå°±èƒ½å¾—åˆ°ä¸€ä¸ªè¯¥ç»“æ„ä¸‹çš„æœ€å¥½åˆ†æ•°ï¼Œé‚£å¦‚ä½•ç¡®å®šæ ‘çš„ç»“æ„å‘¢?
ä¸€ä¸ªæƒ³å½“ç„¶çš„æ–¹æ³•æ˜¯: ä¸æ–­åœ°æšä¸¾ä¸åŒæ ‘çš„ç»“æ„ï¼Œç„¶ååˆ©ç”¨æ‰“åˆ†å‡½æ•°æ¥å¯»æ‰¾å‡ºä¸€ä¸ªæœ€ä¼˜ç»“æ„çš„æ ‘ï¼Œæ¥ çœ‹åŠ å…¥åˆ°æ¨¡å‹ä¸­ï¼Œä¸æ–­é‡å¤è¿™æ ·çš„æ“ä½œã€‚è€Œå†ä¸€æƒ³ï¼Œä½ ä¼šæ„è¯†åˆ°è¦æšä¸¾çš„çŠ¶æ€å¤ªå¤šäº†ï¼ŒåŸºæœ¬å±äºæ— ç©· ç§ï¼Œé‚£å’‹åŠå‘¢?
**è´ªå¿ƒæ³•**:  ä»æ ‘æ·±åº¦ 0 å¼€å§‹ï¼Œæ¯ä¸€èŠ‚ç‚¹éƒ½éå†æ‰€æœ‰çš„ç‰¹å¾ï¼Œæ¯”å¦‚å¹´é¾„ã€æ€§åˆ«ç­‰ç­‰ï¼Œç„¶åå¯¹äºæŸ ä¸ªç‰¹å¾ï¼Œ**å…ˆæŒ‰ç…§è¯¥ç‰¹å¾é‡Œçš„å€¼è¿›è¡Œæ’åº**ï¼Œç„¶**åçº¿æ€§æ‰«æè¯¥ç‰¹å¾è¿›è€Œç¡®å®šæœ€å¥½çš„åˆ†å‰²ç‚¹**ï¼Œæœ€åå¯¹æ‰€æœ‰ç‰¹ å¾è¿›è¡Œåˆ†å‰²åï¼Œ

æ¯”å¦‚æ€»å…±äº”ä¸ªäººï¼ŒæŒ‰å¹´é¾„æ’å¥½åºåï¼Œä¸€å¼€å§‹æˆ‘ä»¬æ€»å…±æœ‰å¦‚ä¸‹4ç§åˆ’åˆ†æ–¹æ³•ï¼š

1. æŠŠç¬¬ä¸€ä¸ªäººå’Œåé¢å››ä¸ªäººåˆ’åˆ†å¼€
2. æŠŠå‰ä¸¤ä¸ªäººå’Œåé¢ä¸‰ä¸ªäººåˆ’åˆ†å¼€
3. æŠŠå‰ä¸‰ä¸ªäººå’Œåé¢ä¸¤ä¸ªäººåˆ’åˆ†å¼€
4. æŠŠå‰é¢å››ä¸ªäººå’Œåé¢ä¸€ä¸ªäººåˆ’åˆ†å¼€

æˆ‘ä»¬é€‰æ‹©æ‰€è°“çš„å¢ç›ŠGainæœ€é«˜çš„é‚£ä¸ªç‰¹å¾ï¼Œè€ŒGainå¦‚ä½•è®¡ç®—å‘¢? we have following object function: 
$$
O b j=-\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda}+\gamma T
$$
æ¢å¥è¯è¯´ï¼Œç›®æ ‡å‡½æ•°ä¸­çš„ $\mathrm{G} /(\mathrm{H}+\lambda)$ éƒ¨åˆ†ï¼Œè¡¨ç¤ºç€æ¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹å¯¹å½“å‰æ¨¡å‹æŸå¤±çš„è´¡çŒ®ç¨‹åº¦ï¼Œèåˆä¸€ ä¸‹ï¼Œå¾—åˆ°Gainçš„è®¡ç®—è¡¨è¾¾å¼ï¼Œå¦‚ä¸‹æ‰€ç¤º:

![img](/assets/Gradient%20boosting.assets/20160421110908655.png)

![img](/assets/Gradient%20boosting.assets/20160421111024891.png)

å› ä¸ºæ¯æ¬¡åˆ†å‰²éƒ½æœ‰$\hat{c}_1=\operatorname{ave}\left(y_i \mid x_i \in R_1(j, s)\right) \text { å’Œ } \hat{c}_2=\operatorname{ave}\left(y_i \mid x_i \in R_2(j, s)\right)$ å¾—åˆ°å…¶ä¸­çš„$\hat{y}_i$é‚£ä¹ˆå°±å¯ä»¥è®¡ç®—å®ƒçš„$g_i=l^{\prime}\left(y_i, \widehat{y}_i^{(t-1)}\right)= - 2(y_i-\hat{y}^{(t-1)}_i)$ åŒæ ·å¤„ç†$h_i$

**å¯¹äºæ‰€æœ‰çš„ç‰¹å¾x**ï¼Œæˆ‘ä»¬åªè¦åšä¸€éä»å·¦åˆ°å³çš„æ‰«æå°±å¯ä»¥æšä¸¾å‡ºæ‰€æœ‰åˆ†å‰²çš„æ¢¯åº¦å’ŒGLå’ŒGRã€‚ç„¶åç”¨è®¡ç®—Gainçš„å…¬å¼è®¡ç®—æ¯ä¸ªåˆ†å‰²æ–¹æ¡ˆçš„åˆ†æ•°å°±å¯ä»¥äº†ã€‚ç„¶ååç»­åˆ™ä¾ç„¶æŒ‰ç…§è¿™ç§åˆ’åˆ†æ–¹æ³•ç»§ç»­ç¬¬äºŒå±‚ã€ç¬¬ä¸‰å±‚ã€ç¬¬å››å±‚ã€ç¬¬Nå±‚çš„åˆ†è£‚ã€‚

ç¬¬äºŒä¸ªå€¼å¾—æ³¨æ„çš„äº‹æƒ…å°±æ˜¯å¼•å…¥åˆ†å‰²ä¸ä¸€å®šä¼šä½¿å¾—æƒ…å†µå˜å¥½ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰ä¸€ä¸ªå¼•å…¥æ–°å¶å­çš„æƒ©ç½šé¡¹ã€‚ä¼˜åŒ–è¿™ä¸ªç›®æ ‡å¯¹åº”äº†æ ‘çš„å‰ªæï¼Œ å½“å¼•å…¥çš„åˆ†å‰²å¸¦æ¥çš„å¢ç›Šå°äºä¸€ä¸ªé˜€å€¼Î³ çš„æ—¶å€™ï¼Œåˆ™å¿½ç•¥è¿™ä¸ªåˆ†å‰²

![img](Machine_LearningNote.assets/20170228144201588.png)

##### (2) Approximate Algorithm è¿‘ä¼¼ç®—æ³•

å¯¹äºè¿ç»­å‹ç‰¹å¾å€¼ï¼Œå½“æ ·æœ¬æ•°é‡éå¸¸å¤§ï¼Œè¯¥ç‰¹å¾å–å€¼è¿‡å¤šæ—¶ï¼Œéå†æ‰€æœ‰å–å€¼ä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚**å› æ­¤XGBoostæ€æƒ³æ˜¯å¯¹ç‰¹å¾è¿›è¡Œåˆ†æ¡¶ï¼Œå³æ‰¾åˆ°$l$ä¸ªåˆ’åˆ†ç‚¹ï¼Œå°†ä½äºç›¸é‚»åˆ†ä½ç‚¹ä¹‹é—´çš„æ ·æœ¬åˆ†åœ¨ä¸€ä¸ªæ¡¶ä¸­**ã€‚åœ¨éå†è¯¥ç‰¹å¾çš„æ—¶å€™ï¼Œåªéœ€è¦éå†å„ä¸ªåˆ†ä½ç‚¹ï¼Œä»è€Œè®¡ç®—æœ€ä¼˜åˆ’åˆ†ã€‚

ä»ç®—æ³•ä¼ªä»£ç ä¸­è¯¥æµç¨‹è¿˜å¯ä»¥åˆ†ä¸ºä¸¤ç§ï¼š

å…¨å±€çš„è¿‘ä¼¼: æ˜¯åœ¨æ–°ç”Ÿæˆä¸€æ£µæ ‘ä¹‹å‰å°±å¯¹å„ä¸ªç‰¹å¾è®¡ç®—åˆ†ä½ç‚¹å¹¶åˆ’åˆ†æ ·æœ¬ï¼Œä¹‹ååœ¨æ¯æ¬¡åˆ†è£‚è¿‡ç¨‹ä¸­éƒ½é‡‡ç”¨è¿‘ä¼¼åˆ’åˆ†
å±€éƒ¨è¿‘ä¼¼: å°±æ˜¯åœ¨å…·ä½“çš„æŸä¸€æ¬¡åˆ†è£‚èŠ‚ç‚¹çš„è¿‡ç¨‹ä¸­é‡‡ç”¨è¿‘ä¼¼ç®—æ³•ã€‚



![img](/assets/Gradient%20boosting.assets/20170228144525979.png)

- **ç¬¬ä¸€ä¸ª for å¾ªç¯ï¼š**å¯¹ç‰¹å¾ k æ ¹æ®è¯¥ç‰¹å¾åˆ†å¸ƒçš„åˆ†ä½æ•°æ‰¾åˆ°åˆ‡å‰²ç‚¹çš„å€™é€‰é›†åˆ $S_k=\left\{s_{k 1}, s_{k 2}, \ldots, s_{k l}\right\}$ã€‚XGBoost æ”¯æŒ Global ç­–ç•¥å’Œ Local ç­–ç•¥ã€‚
- **ç¬¬äºŒä¸ª for å¾ªç¯ï¼š**é’ˆå¯¹æ¯ä¸ªç‰¹å¾çš„å€™é€‰é›†åˆï¼Œå°†æ ·æœ¬æ˜ å°„åˆ°ç”±è¯¥ç‰¹å¾å¯¹åº”çš„å€™é€‰ç‚¹é›†æ„æˆçš„åˆ†æ¡¶åŒºé—´ä¸­ï¼Œå³ $s_{k, v} \geq x_{j k}>s_{k, v-1}$  ï¼Œå¯¹æ¯ä¸ªæ¡¶ç»Ÿè®¡ $G$, $H$å€¼ï¼Œæœ€ååœ¨è¿™äº›ç»Ÿè®¡é‡ä¸Šå¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹ã€‚

ä¸‹å›¾ç»™å‡ºè¿‘ä¼¼ç®—æ³•çš„å…·ä½“ä¾‹å­ï¼Œä»¥ä¸‰åˆ†ä½ä¸ºä¾‹ï¼š

![img](/assets/Gradient%20boosting.assets/v2-5d1dd1673419599094bf44dd4b533ba9_720w.webp)

æ ¹æ®æ ·æœ¬ç‰¹å¾è¿›è¡Œæ’åºï¼Œç„¶ååŸºäºåˆ†ä½æ•°è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶ç»Ÿè®¡ä¸‰ä¸ªæ¡¶å†…çš„ $G$, $H$å€¼ï¼Œæœ€ç»ˆæ±‚è§£èŠ‚ç‚¹åˆ’åˆ†çš„å¢ç›Šã€‚

äº‹å®ä¸Šï¼Œ XGBoost ä¸æ˜¯ç®€å•åœ°æŒ‰ç…§æ ·æœ¬ä¸ªæ•°è¿›è¡Œåˆ†ä½ï¼Œè€Œæ˜¯ä»¥äºŒé˜¶å¯¼æ•°å€¼ $â„_i$ ä½œä¸ºæ ·æœ¬çš„æƒé‡è¿›è¡Œåˆ’åˆ†ï¼Œå¦‚ä¸‹ï¼š

![img](/assets/Gradient%20boosting.assets/v2-5f16246289eaa2a3ae72f971db198457_720w.webp)
$$
\begin{aligned}
O b j^{(t)} & \approx \sum_{i=1}^n\left[g_i f_t\left(x_i\right)+\frac{1}{2} h_i f_t^2\left(x_i\right)\right]+\sum_{i=1}^t \Omega\left(f_i\right) \\
& =\sum_{i=1}^n\left[g_i f_t\left(x_i\right)+\frac{1}{2} h_i f_t^2\left(x_i\right)+\frac{1}{2} \frac{g_i^2}{h_i}\right]+\Omega\left(f_t\right)+C \\
& =\sum_{i=1}^n \frac{1}{2} h_i\left[f_t\left(x_i\right)-\left(-\frac{g_i}{h_i}\right)\right]^2+\Omega\left(f_t\right)+C
\end{aligned}
$$
å…¶ä¸­ $\frac{1}{2} \frac{g_i^2}{h_i}$ ä¸ $C$ çš†ä¸ºå¸¸æ•°ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ° $h_i$ å°±æ˜¯å¹³æ–¹æŸå¤±å‡½æ•°ä¸­æ ·æœ¬çš„æƒé‡ã€‚

å¯¹äºæ ·æœ¬æƒå€¼ç›¸åŒçš„æ•°æ®é›†æ¥è¯´ï¼Œæ‰¾åˆ°å€™é€‰åˆ†ä½ç‚¹å·²ç»æœ‰äº†è§£å†³æ–¹æ¡ˆï¼ˆGK ç®—æ³•ï¼‰ï¼Œä½†æ˜¯å½“æ ·æœ¬æƒå€¼ä¸ä¸€æ ·æ—¶ï¼Œè¯¥å¦‚ä½•æ‰¾åˆ°å€™é€‰åˆ†ä½ç‚¹å‘¢ï¼Ÿä½œè€…ç»™å‡ºäº†ä¸€ä¸ª Weighted Quantile Sketch ç®—æ³•

#### **é’ˆå¯¹ç¨€ç–æ•°æ®çš„ç®—æ³•â€”â€”ç¼ºå¤±å€¼å¤„ç†**

å½“æ ·æœ¬çš„ç¬¬iä¸ªç‰¹å¾å€¼ç¼ºå¤±æ—¶ï¼Œæ— æ³•åˆ©ç”¨è¯¥ç‰¹å¾è¿›è¡Œåˆ’åˆ†æ—¶ï¼ŒXGBoostçš„æƒ³æ³•æ˜¯å°†è¯¥æ ·æœ¬åˆ†åˆ«åˆ’åˆ†åˆ°å·¦ç»“ç‚¹å’Œå³ç»“ç‚¹ï¼Œç„¶åè®¡ç®—å…¶å¢ç›Šï¼Œå“ªä¸ªå¤§å°±åˆ’åˆ†åˆ°å“ªè¾¹ã€‚å…·ä½“è§ç®—æ³•3

<img src="/assets/Gradient%20boosting.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VtbWFfTG92ZQ==,size_16,color_FFFFFF,t_70-1685936352265-73.png" alt="img" style="zoom: 50%;" />

#### Boosted Tree Algorithm 

![img](/assets/Gradient%20boosting.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZfSlVMWV92,size_16,color_FFFFFF,t_70-1685885343558-67.png)

####  XGBoost vs. GBDT

1. ä¼ ç»Ÿçš„GBDTä»¥CARTæ ‘ä½œä¸ºåŸºå­¦ä¹ å™¨ï¼ŒXGBoostè¿˜æ”¯æŒçº¿æ€§åˆ†ç±»å™¨ï¼Œè¿™ä¸ªæ—¶å€™XGBoostç›¸å½“äºL1å’ŒL2æ­£åˆ™åŒ–çš„é€»è¾‘æ–¯è’‚å›å½’ï¼ˆåˆ†ç±»ï¼‰æˆ–è€…çº¿æ€§å›å½’ï¼ˆå›å½’ï¼‰ï¼›
2. ä¼ ç»Ÿçš„GBDTåœ¨ä¼˜åŒ–çš„æ—¶å€™åªç”¨åˆ°ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼ŒXGBooståˆ™å¯¹ä»£ä»·å‡½æ•°è¿›è¡Œäº†äºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¾—åˆ°ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°
3. XGBooståœ¨ä»£ä»·å‡½æ•°ä¸­åŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚ä»æƒè¡¡æ–¹å·®åå·®æ¥çœ‹ï¼Œå®ƒé™ä½äº†æ¨¡å‹çš„æ–¹å·®ï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œæ”¾ç½®è¿‡æ‹Ÿåˆï¼Œè¿™ä¹Ÿæ˜¯XGBoostä¼˜äºä¼ ç»ŸGBDTçš„ä¸€ä¸ªç‰¹æ€§ï¼›
4. shrinkageï¼ˆç¼©å‡ï¼‰ï¼Œç›¸å½“äºå­¦ä¹ é€Ÿç‡ï¼ˆXGBoostä¸­çš„etaï¼‰ã€‚XGBooståœ¨è¿›è¡Œå®Œä¸€æ¬¡è¿­ä»£æ—¶ï¼Œä¼šå°†å¶å­èŠ‚ç‚¹çš„æƒå€¼ä¹˜ä¸Šè¯¥ç³»æ•°ï¼Œä¸»è¦æ˜¯ä¸ºäº†å‰Šå¼±æ¯æ£µæ ‘çš„å½±å“ï¼Œè®©åé¢æœ‰æ›´å¤§çš„å­¦ä¹ ç©ºé—´ã€‚ï¼ˆGBDTä¹Ÿæœ‰å­¦ä¹ é€Ÿç‡ï¼‰ï¼›
5. Column Subsamplingåˆ—æŠ½æ ·ã€‚XGBoostå€Ÿé‰´äº†éšæœºæ£®æ—çš„åšæ³•ï¼Œæ”¯æŒåˆ—æŠ½æ ·ï¼Œä¸ä»…é˜²æ­¢è¿‡ æ‹Ÿåˆï¼Œè¿˜èƒ½å‡å°‘è®¡ç®—ï¼›
6. å¯¹ç¼ºå¤±å€¼çš„å¤„ç†ã€‚å¯¹äºç‰¹å¾çš„å€¼æœ‰ç¼ºå¤±çš„æ ·æœ¬ï¼ŒXGBoostè¿˜å¯ä»¥è‡ªåŠ¨ å­¦ä¹ å‡ºå®ƒçš„åˆ†è£‚æ–¹å‘ï¼›
7. XGBoostå·¥å…·æ”¯æŒå¹¶è¡Œã€‚Boostingä¸æ˜¯ä¸€ç§ä¸²è¡Œçš„ç»“æ„å—?æ€ä¹ˆå¹¶è¡Œ çš„ï¼Ÿæ³¨æ„XGBoostçš„å¹¶è¡Œä¸æ˜¯treeç²’åº¦çš„å¹¶è¡Œï¼ŒXGBoostä¹Ÿæ˜¯ä¸€æ¬¡è¿­ä»£å®Œæ‰èƒ½è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£çš„ï¼ˆç¬¬tæ¬¡è¿­ä»£çš„ä»£ä»·å‡½æ•°é‡ŒåŒ…å«äº†å‰é¢t-1æ¬¡è¿­ä»£çš„é¢„æµ‹å€¼ï¼‰ã€‚**XGBoostçš„å¹¶è¡Œæ˜¯åœ¨ç‰¹å¾ç²’åº¦ä¸Šçš„**ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œå†³ç­–æ ‘çš„å­¦ä¹ æœ€è€—æ—¶çš„ä¸€ä¸ªæ­¥éª¤å°±æ˜¯å¯¹ç‰¹å¾çš„å€¼è¿›è¡Œæ’åºï¼ˆå› ä¸ºè¦ç¡®å®šæœ€ä½³åˆ†å‰²ç‚¹ï¼‰ï¼Œ**XGBooståœ¨è®­ç»ƒä¹‹å‰ï¼Œé¢„å…ˆå¯¹æ•°æ®è¿›è¡Œäº†æ’åºï¼Œç„¶åä¿å­˜ä¸ºblockç»“æ„**ï¼Œåé¢çš„è¿­ä»£ ä¸­é‡å¤åœ°ä½¿ç”¨è¿™ä¸ªç»“æ„ï¼Œå¤§å¤§å‡å°è®¡ç®—é‡ã€‚è¿™ä¸ªblockç»“æ„ä¹Ÿä½¿å¾—å¹¶è¡Œæˆä¸ºäº†å¯èƒ½ï¼Œåœ¨è¿›è¡ŒèŠ‚ç‚¹çš„åˆ†è£‚æ—¶ï¼Œéœ€è¦è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¢ç›Šï¼Œæœ€ç»ˆé€‰å¢ç›Šæœ€å¤§çš„é‚£ä¸ªç‰¹å¾å»åšåˆ†è£‚ï¼Œé‚£ä¹ˆå„ä¸ªç‰¹å¾çš„å¢ç›Šè®¡ç®—å°±å¯ä»¥å¼€å¤šçº¿ç¨‹è¿›è¡Œ



## LightGBM

LightGBMï¼ˆLight Gradient Boosting Machineï¼‰æ˜¯ä¸€ä¸ªå®ç°GBDTç®—æ³•çš„æ¡†æ¶ï¼Œæ”¯æŒé«˜æ•ˆç‡çš„å¹¶è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å…·æœ‰æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€æ›´ä½çš„å†…å­˜æ¶ˆè€—ã€æ›´å¥½çš„å‡†ç¡®ç‡ã€æ”¯æŒåˆ†å¸ƒå¼å¯ä»¥å¿«é€Ÿå¤„ç†æµ·é‡æ•°æ®ç­‰ä¼˜ç‚¹

- **èµ„æ–™ï¼š**

[LightGBM: A Highly Efficient Gradient Boosting Decision Tree (neurips.cc)](https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)

A communication-efficient parallel algorithm for decision treeï¼š[1611.01276.pdf (arxiv.org)](https://arxiv.org/pdf/1611.01276.pdf)

[æ·±å…¥ç†è§£LightGBM - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/99069186)

[ã€æœºå™¨å­¦ä¹ ã€‘å†³ç­–æ ‘ï¼ˆä¸‹ï¼‰â€”â€”XGBoostã€LightGBMï¼ˆéå¸¸è¯¦ç»†ï¼‰ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/87885678)

- **LightGBMæå‡ºçš„åŠ¨æœº**

å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œç­‰ç®—æ³•ï¼Œéƒ½å¯ä»¥ä»¥mini-batchçš„æ–¹å¼è®­ç»ƒï¼Œè®­ç»ƒæ•°æ®çš„å¤§å°ä¸ä¼šå—åˆ°å†…å­˜é™åˆ¶ã€‚è€ŒGBDTåœ¨æ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œéƒ½éœ€è¦éå†æ•´ä¸ªè®­ç»ƒæ•°æ®å¤šæ¬¡ã€‚å¦‚æœæŠŠæ•´ä¸ªè®­ç»ƒæ•°æ®è£…è¿›å†…å­˜åˆ™ä¼šé™åˆ¶è®­ç»ƒæ•°æ®çš„å¤§å°ï¼›å¦‚æœä¸è£…è¿›å†…å­˜ï¼Œåå¤åœ°è¯»å†™è®­ç»ƒæ•°æ®åˆä¼šæ¶ˆè€—éå¸¸å¤§çš„æ—¶é—´ã€‚å°¤å…¶é¢å¯¹å·¥ä¸šçº§æµ·é‡çš„æ•°æ®ï¼Œæ™®é€šçš„GBDTç®—æ³•æ˜¯ä¸èƒ½æ»¡è¶³å…¶éœ€æ±‚çš„ã€‚

LightGBMæå‡ºçš„ä¸»è¦åŸå› å°±æ˜¯ä¸ºäº†è§£å†³GBDTåœ¨æµ·é‡æ•°æ®é‡åˆ°çš„é—®é¢˜ï¼Œè®©GBDTå¯ä»¥æ›´å¥½æ›´å¿«åœ°ç”¨äºå·¥ä¸šå®è·µã€‚

- **XGBoostçš„ç¼ºç‚¹åŠLightGBMçš„ä¼˜åŒ–**

åœ¨LightGBMæå‡ºä¹‹å‰ï¼Œæœ€æœ‰åçš„GBDTå·¥å…·å°±æ˜¯XGBoostäº†ï¼Œå®ƒæ˜¯åŸºäºé¢„æ’åºæ–¹æ³•çš„å†³ç­–æ ‘ç®—æ³•ã€‚è¿™ç§æ„å»ºå†³ç­–æ ‘çš„ç®—æ³•åŸºæœ¬æ€æƒ³æ˜¯ï¼š**é¦–å…ˆï¼Œå¯¹æ‰€æœ‰ç‰¹å¾éƒ½æŒ‰ç…§ç‰¹å¾çš„æ•°å€¼è¿›è¡Œé¢„æ’åº**ã€‚å…¶æ¬¡ï¼Œåœ¨éå†åˆ†å‰²ç‚¹çš„æ—¶å€™ç”¨O(#data)çš„ä»£ä»·æ‰¾åˆ°ä¸€ä¸ªç‰¹å¾ä¸Šçš„æœ€å¥½åˆ†å‰²ç‚¹ã€‚æœ€åï¼Œåœ¨æ‰¾åˆ°ä¸€ä¸ªç‰¹å¾çš„æœ€å¥½åˆ†å‰²ç‚¹åï¼Œå°†æ•°æ®åˆ†è£‚æˆå·¦å³å­èŠ‚ç‚¹ã€‚

è¿™æ ·çš„é¢„æ’åºç®—æ³•çš„ä¼˜ç‚¹æ˜¯èƒ½ç²¾ç¡®åœ°æ‰¾åˆ°åˆ†å‰²ç‚¹ã€‚ä½†æ˜¯ç¼ºç‚¹ä¹Ÿå¾ˆæ˜æ˜¾ï¼šé¦–å…ˆï¼Œç©ºé—´æ¶ˆè€—å¤§ã€‚è¿™æ ·çš„ç®—æ³•éœ€è¦ä¿å­˜æ•°æ®çš„ç‰¹å¾å€¼ï¼Œè¿˜ä¿å­˜äº†ç‰¹å¾æ’åºçš„ç»“æœï¼ˆä¾‹å¦‚ï¼Œä¸ºäº†åç»­å¿«é€Ÿçš„è®¡ç®—åˆ†å‰²ç‚¹ï¼Œä¿å­˜äº†æ’åºåçš„ç´¢å¼•ï¼‰ï¼Œè¿™å°±éœ€è¦æ¶ˆè€—è®­ç»ƒæ•°æ®ä¸¤å€çš„å†…å­˜ã€‚å…¶æ¬¡ï¼Œæ—¶é—´ä¸Šä¹Ÿæœ‰è¾ƒå¤§çš„å¼€é”€ï¼Œåœ¨éå†æ¯ä¸€ä¸ªåˆ†å‰²ç‚¹çš„æ—¶å€™ï¼Œéƒ½éœ€è¦è¿›è¡Œåˆ†è£‚å¢ç›Šçš„è®¡ç®—ï¼Œæ¶ˆè€—çš„ä»£ä»·å¤§ã€‚æœ€åï¼Œå¯¹cacheä¼˜åŒ–ä¸å‹å¥½ã€‚åœ¨é¢„æ’åºåï¼Œç‰¹å¾å¯¹æ¢¯åº¦çš„è®¿é—®æ˜¯ä¸€ç§éšæœºè®¿é—®ï¼Œå¹¶ä¸”ä¸åŒçš„ç‰¹å¾è®¿é—®çš„é¡ºåºä¸ä¸€æ ·ï¼Œæ— æ³•å¯¹cacheè¿›è¡Œä¼˜åŒ–ã€‚åŒæ—¶ï¼Œåœ¨æ¯ä¸€å±‚é•¿æ ‘çš„æ—¶å€™ï¼Œéœ€è¦éšæœºè®¿é—®ä¸€ä¸ªè¡Œç´¢å¼•åˆ°å¶å­ç´¢å¼•çš„æ•°ç»„ï¼Œå¹¶ä¸”ä¸åŒç‰¹å¾è®¿é—®çš„é¡ºåºä¹Ÿä¸ä¸€æ ·ï¼Œä¹Ÿä¼šé€ æˆè¾ƒå¤§çš„cache missã€‚

###  **LightGBMçš„åŸºæœ¬åŸç†**

[ã€ç™½è¯æœºå™¨å­¦ä¹ ã€‘ç®—æ³•ç†è®º+å®æˆ˜ä¹‹LightGBMç®—æ³•-è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº-è…¾è®¯äº‘ (tencent.com)](https://cloud.tencent.com/developer/article/1651704)

Histogram algorithmåº”è¯¥ç¿»è¯‘ä¸ºç›´æ–¹å›¾ç®—æ³•ï¼Œç›´æ–¹å›¾ç®—æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šå…ˆæŠŠè¿ç»­çš„æµ®ç‚¹ç‰¹å¾å€¼ç¦»æ•£åŒ–æˆ K ä¸ªæ•´æ•°ï¼Œæ¯”å¦‚[0, 0.1) ->0, [0.1, 0.3)->1ï¼Œ åŒæ—¶æ„é€ ä¸€ä¸ªå®½åº¦ä¸º k çš„ç›´æ–¹å›¾ç”¨äºç»Ÿè®¡ä¿¡æ¯ï¼ˆå«æœ‰ k ä¸ª binï¼‰ã€‚åœ¨éå†æ•°æ®çš„æ—¶å€™ï¼Œæ ¹æ®ç¦»æ•£åŒ–åçš„å€¼ä½œä¸ºç´¢å¼•åœ¨ç›´æ–¹å›¾ä¸­ç´¯ç§¯ç»Ÿè®¡é‡ï¼Œå½“éå†ä¸€æ¬¡æ•°æ®åï¼Œç›´æ–¹å›¾ç´¯ç§¯äº†éœ€è¦çš„ç»Ÿè®¡é‡ï¼Œç„¶åæ ¹æ®ç›´æ–¹å›¾çš„ç¦»æ•£å€¼ï¼ˆ k ä¸ª bin ï¼‰ï¼Œéå†å¯»æ‰¾æœ€ä¼˜çš„åˆ†å‰²ç‚¹ã€‚

- å†…å­˜å ç”¨æ›´å°: XGBoost éœ€è¦ç”¨ 32 ä½çš„æµ®ç‚¹æ•°å»å­˜å‚¨ç‰¹å¾å€¼ï¼Œå¹¶ç”¨ 32 ä½çš„æ•´å½¢å»å­˜å‚¨ç´¢å¼•ï¼Œ è€Œ LightGBM åªéœ€è¦ç”¨ 8 ä½å»å­˜å‚¨ç›´æ–¹å›¾ï¼Œç›¸å½“äºå‡å°‘äº† $1 / 8$;

- è®¡ç®—ä»£ä»·æ›´å°: è®¡ç®—ç‰¹å¾åˆ†è£‚å¢ç›Šæ—¶ï¼ŒXGBoost éœ€è¦éå†ä¸€æ¬¡æ•°æ®æ‰¾åˆ°æœ€ä½³åˆ†è£‚ç‚¹ï¼Œè€Œ Light $G B M$ åªéœ€è¦éå†ä¸€æ¬¡ $\mathrm{k}$ æ¬¡ï¼Œç›´æ¥å°†æ—¶é—´å¤æ‚åº¦ä» $O(\#$ data * \#feature $)$ é™ä½åˆ° $O(k * \#$ feature $)$ ï¼Œè€Œæˆ‘ä»¬çŸ¥é“ $\#$ data $>>k$ ã€‚

  

  <img src="/assets/Gradient%20boosting.assets/1200.png" alt="img" style="zoom:50%;" />



![img](/assets/Gradient%20boosting.assets/1200-1685938257159-78.png)

### **ç›´æ–¹å›¾ä½œå·®åŠ é€Ÿ**

å½“èŠ‚ç‚¹åˆ†è£‚æˆä¸¤ä¸ªæ—¶ï¼Œå³è¾¹çš„å­èŠ‚ç‚¹çš„ç›´æ–¹å›¾å…¶å®ç­‰äºå…¶çˆ¶èŠ‚ç‚¹çš„ç›´æ–¹å›¾å‡å»å·¦è¾¹å­èŠ‚ç‚¹çš„ç›´æ–¹å›¾ï¼š

![img](/assets/Gradient%20boosting.assets/1200-1685938350319-81.png)

![img](/assets/Gradient%20boosting.assets/1200-1685938371956-84.png)

- ç¦»æ•£åŒ–çš„åˆ†è£‚ç‚¹å¯¹æœ€ç»ˆçš„ç²¾åº¦å½±å“å¹¶ä¸å¤§ï¼Œç”šè‡³ä¼šå¥½ä¸€äº›ã€‚åŸå› åœ¨äºdecision treeæœ¬èº«å°±æ˜¯ä¸€ä¸ªå¼±å­¦ä¹ å™¨ï¼Œåˆ†å‰²ç‚¹æ˜¯ä¸æ˜¯ç²¾ç¡®å¹¶ä¸æ˜¯å¤ªé‡è¦ï¼Œé‡‡ç”¨Histogramç®—æ³•ä¼šèµ·åˆ°æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œæœ‰æ•ˆåœ°é˜²æ­¢æ¨¡å‹çš„è¿‡æ‹Ÿåˆï¼ˆbinæ•°é‡å†³å®šäº†æ­£åˆ™åŒ–çš„ç¨‹åº¦ï¼Œbinè¶Šå°‘æƒ©ç½šè¶Šä¸¥é‡ï¼Œæ¬ æ‹Ÿåˆé£é™©è¶Šé«˜ï¼‰ã€‚

- ç›´æ–¹å›¾ç®—æ³•å¯ä»¥èµ·åˆ°çš„ä½œç”¨å°±æ˜¯å¯ä»¥å‡å°åˆ†å‰²ç‚¹çš„æ•°é‡ï¼Œ åŠ å¿«è®¡ç®—ã€‚

### **å¸¦æ·±åº¦é™åˆ¶çš„ Leaf-wise ç®—æ³•**

LightGBMè¿›è¡Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–ã€‚é¦–å…ˆå®ƒæŠ›å¼ƒäº†å¤§å¤šæ•°GBDTå·¥å…·ä½¿ç”¨çš„æŒ‰å±‚ç”Ÿé•¿ (level-wise) çš„å†³ç­–æ ‘ç”Ÿé•¿ç­–ç•¥ï¼Œè€Œä½¿ç”¨äº†å¸¦æœ‰æ·±åº¦é™åˆ¶çš„æŒ‰å¶å­ç”Ÿé•¿ (leaf-wise) ç®—æ³•

XGBoost é‡‡ç”¨ Level-wise çš„å¢é•¿ç­–ç•¥ï¼Œè¯¥ç­–ç•¥éå†ä¸€æ¬¡æ•°æ®å¯ä»¥åŒæ—¶åˆ†è£‚åŒä¸€å±‚çš„å¶å­ï¼Œå®¹æ˜“è¿›è¡Œå¤šçº¿ç¨‹ä¼˜åŒ–ï¼Œä¹Ÿå¥½æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä½†å®é™…ä¸ŠLevel-wiseæ˜¯ä¸€ç§ä½æ•ˆçš„ç®—æ³•ï¼Œå› ä¸ºå®ƒä¸åŠ åŒºåˆ†çš„å¯¹å¾…åŒä¸€å±‚çš„å¶å­ï¼Œå®é™…ä¸Šå¾ˆå¤šå¶å­çš„åˆ†è£‚å¢ç›Šè¾ƒä½ï¼Œæ²¡å¿…è¦è¿›è¡Œæœç´¢å’Œåˆ†è£‚ï¼Œå› æ­¤å¸¦æ¥äº†å¾ˆå¤šæ²¡å¿…è¦çš„è®¡ç®—å¼€é”€

![img](/assets/Gradient%20boosting.assets/v2-79a074ec2964a82301209fb66df37113_720w.webp)

LightGBMé‡‡ç”¨Leaf-wiseçš„å¢é•¿ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ¯æ¬¡ä»å½“å‰æ‰€æœ‰å¶å­ä¸­ï¼Œæ‰¾åˆ°åˆ†è£‚å¢ç›Šæœ€å¤§çš„ä¸€ä¸ªå¶å­ï¼Œç„¶ååˆ†è£‚ï¼Œå¦‚æ­¤å¾ªç¯ã€‚å› æ­¤åŒLevel-wiseç›¸æ¯”ï¼Œ**Leaf-wiseçš„ä¼˜ç‚¹æ˜¯ï¼šåœ¨åˆ†è£‚æ¬¡æ•°ç›¸åŒçš„æƒ…å†µä¸‹ï¼ŒLeaf-wiseå¯ä»¥é™ä½æ›´å¤šçš„è¯¯å·®ï¼Œå¾—åˆ°æ›´å¥½çš„ç²¾åº¦ï¼›Leaf-wiseçš„ç¼ºç‚¹æ˜¯ï¼šå¯èƒ½ä¼šé•¿å‡ºæ¯”è¾ƒæ·±çš„å†³ç­–æ ‘ï¼Œäº§ç”Ÿè¿‡æ‹Ÿåˆ**ã€‚å› æ­¤LightGBMä¼šåœ¨Leaf-wiseä¹‹ä¸Šå¢åŠ äº†ä¸€ä¸ªæœ€å¤§æ·±åº¦çš„é™åˆ¶ï¼Œåœ¨ä¿è¯é«˜æ•ˆç‡çš„åŒæ—¶é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

![img](/assets/Gradient%20boosting.assets/v2-e762a7e4c0366003d7f82e1817da9f89_r.jpg)

### **å•è¾¹æ¢¯åº¦æŠ½æ ·ç®—æ³•(GOSS)**

Gradient-based One-Side Sampling, 

åœ¨AdaBoostä¸­ï¼Œä¼šç»™æ¯ä¸ªæ ·æœ¬ä¸€ä¸ªæƒé‡ï¼Œç„¶åæ¯ä¸€è½®ä¹‹åè°ƒå¤§é”™è¯¯æ ·æœ¬çš„æƒé‡ï¼Œè®©åé¢çš„æ¨¡å‹æ›´åŠ å…³æ³¨å‰é¢é”™è¯¯åŒºåˆ†çš„æ ·æœ¬ï¼Œè¿™æ—¶å€™æ ·æœ¬æƒé‡æ˜¯æ•°æ®é‡è¦æ€§çš„æ ‡å¿—ï¼Œåˆ°äº†GBDTä¸­ï¼Œ ç¡®å®æ²¡æœ‰ä¸€ä¸ªåƒAdaboosté‡Œé¢è¿™æ ·çš„æ ·æœ¬æƒé‡ï¼Œç†è®ºä¸Šè¯´æ˜¯ä¸èƒ½åº”ç”¨æƒé‡è¿›è¡Œé‡‡æ ·çš„ï¼Œ **ä½†æ˜¯GBDTä¸­æ¯ä¸ªæ•°æ®éƒ½ä¼šæœ‰ä¸åŒçš„æ¢¯åº¦å€¼**ï¼Œ è¿™ä¸ªå¯¹é‡‡æ ·æ˜¯ååˆ†æœ‰ç”¨çš„ï¼Œ å³æ¢¯åº¦å°çš„æ ·æœ¬ï¼Œè®­ç»ƒè¯¯å·®ä¹Ÿæ¯”è¾ƒå°ï¼Œè¯´æ˜æ•°æ®å·²ç»è¢«æ¨¡å‹å­¦ä¹ çš„å¾ˆå¥½äº†ï¼Œå› ä¸ºGBDTèšç„¦æ®‹å·® which is negative gradient in function space! **åœ¨è®­ç»ƒæ–°æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦æ¯”è¾ƒå°çš„æ ·æœ¬å¯¹äºé™ä½æ®‹å·®çš„ä½œç”¨æ•ˆæœä¸æ˜¯å¤ªå¤§(residual (gradient) is small , no much room to reduce)ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…³æ³¨æ¢¯åº¦/residual é«˜çš„æ ·æœ¬**ï¼Œwhich is $\nabla_{\hat{\mathbf{y}}} L(\mathbf{y}, \hat{\mathbf{y}})=-2(\mathbf{y}-\hat{\mathbf{y}})$ if using MSE....

GOSS ç®—æ³•ä¿ç•™äº†æ¢¯åº¦å¤§çš„æ ·æœ¬ï¼Œå¹¶å¯¹æ¢¯åº¦å°çš„æ ·æœ¬è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œä¸ºäº†ä¸æ”¹å˜æ ·æœ¬çš„æ•°æ®åˆ†å¸ƒï¼Œ**åœ¨è®¡ç®—å¢ç›Šæ—¶ä¸ºæ¢¯åº¦å°çš„æ ·æœ¬å¼•å…¥ä¸€ä¸ªå¸¸æ•°è¿›è¡Œå¹³è¡¡**ã€‚é¦–å…ˆå°†è¦è¿›è¡Œåˆ†è£‚çš„ç‰¹å¾çš„æ‰€æœ‰å–å€¼æŒ‰ç…§ç»å¯¹å€¼å¤§å°é™åºæ’åº (xgboostä¹Ÿè¿›è¡Œäº†æ’åºï¼Œä½†æ˜¯LightGBMä¸ç”¨ä¿å­˜æ’åºåçš„ç»“æœï¼‰ï¼Œé€‰å–ç»å¯¹å€¼æœ€å¤§çš„ a ä¸ªæ•°æ®ã€‚ç„¶ååœ¨å‰©ä¸‹çš„è¾ƒå°æ¢¯åº¦æ•°æ®ä¸­éšæœºé€‰æ‹© bä¸ªæ•°æ®ã€‚æ¥ç€å°†è¿™ b ä¸ªæ•°æ®ä¹˜ä»¥ä¸€ä¸ªå¹³è¡¡å¸¸æ•°$\frac{1-a}{b}$ ï¼Œè¿™æ ·ç®—æ³•å°±ä¼šæ›´å…³æ³¨è®­ç»ƒä¸è¶³çš„æ ·æœ¬ï¼Œè€Œä¸ä¼šè¿‡å¤šæ”¹å˜åŸæ•°æ®é›†çš„åˆ†å¸ƒã€‚æœ€åä½¿ç”¨è¿™ (a+b) ä¸ªæ•°æ®æ¥è®¡ç®—ä¿¡æ¯å¢ç›Šã€‚ä¸‹å›¾æ˜¯GOSSçš„å…·ä½“ç®—æ³•

![img](/assets/Gradient%20boosting.assets/v2-79c3e6d91863f2512105f86dde65807d_720w.webp)

### **äº’æ–¥ç‰¹å¾æ†ç»‘ç®—æ³•(EFB)**

é«˜ç»´åº¦çš„æ•°æ®å¾€å¾€æ˜¯ç¨€ç–çš„ï¼Œ**è¿™ç§ç¨€ç–æ€§å¯å‘æˆ‘ä»¬è®¾è®¡ä¸€ç§æ— æŸçš„æ–¹æ³•æ¥å‡å°‘ç‰¹å¾çš„ç»´åº¦**ã€‚é€šå¸¸è¢« æ†ç»‘çš„ç‰¹å¾éƒ½æ˜¯äº’æ–¥çš„ (å³ç‰¹å¾ä¸ä¼šåŒæ—¶ä¸ºéé›¶å€¼ï¼Œåƒone-hot)ï¼Œè¿™æ ·ä¸¤ä¸ªç‰¹å¾æ†ç»‘èµ·æ¥æ‰ä¸ä¼š ä¸Ÿå¤±ä¿¡æ¯ã€‚**å¦‚æœä¸¤ä¸ªç‰¹å¾å¹¶ä¸æ˜¯å®Œå…¨äº’æ–¥ï¼ˆéƒ¨åˆ†æƒ…å†µä¸‹ä¸¤ä¸ªç‰¹å¾éƒ½æ˜¯éé›¶å€¼ï¼‰ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªæŒ‡æ ‡å¯¹ ç‰¹å¾ä¸äº’æ–¥ç¨‹åº¦è¿›è¡Œè¡¡é‡ï¼Œç§°ä¹‹ä¸ºå†²çªæ¯”ç‡**ï¼Œå½“è¿™ä¸ªå€¼è¾ƒå°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æŠŠä¸å®Œå…¨äº’æ–¥çš„ä¸¤ä¸ª ç‰¹å¾æ†ç»‘ï¼Œè€Œä¸å½±å“æœ€åçš„ç²¾åº¦ã€‚

**äº’æ–¥ç‰¹å¾æ†ç»‘ç®—æ³• (Exclusive Feature Bundling, EFB) æŒ‡å‡ºå¦‚ æœå°†ä¸€äº›ç‰¹å¾è¿›è¡Œèåˆç»‘å®šï¼Œåˆ™å¯ä»¥é™ä½ç‰¹å¾æ•°é‡ã€‚è¿™æ ·åœ¨æ„å»ºç›´æ–¹å›¾æ—¶çš„æ—¶é—´å¤æ‚åº¦ä» $O(\# d a t a * \#$ feature $)$ å˜ä¸º $O(\#$ data $*$ \#bundle $)$ ï¼Œè¿™é‡Œ \#bundle æŒ‡ç‰¹å¾èåˆç»‘å®šå ç‰¹å¾åŒ…çš„ä¸ªæ•°ï¼Œä¸” \#bundle è¿œå°äº \# featureã€‚**

é’ˆå¯¹è¿™ç§æƒ³æ³•ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°ä¸¤ä¸ªé—®é¢˜:

- æ€ä¹ˆåˆ¤å®šå“ªäº›ç‰¹å¾åº”è¯¥ç»‘åœ¨ä¸€èµ· (build bundled) ?
- æ€ä¹ˆæŠŠç‰¹å¾ç»‘ä¸ºä¸€ä¸ªï¼ˆmerge featureï¼‰ï¼Ÿ



#### **è§£å†³å“ªäº›ç‰¹å¾åº”è¯¥ç»‘åœ¨ä¸€èµ·**

å°†ç›¸äº’ç‹¬ç«‹çš„ç‰¹å¾è¿›è¡Œç»‘å®šæ˜¯ä¸€ä¸ª NP-Hard é—®é¢˜ï¼ŒLightGBMçš„EFBç®—æ³•å°†è¿™ä¸ªé—®é¢˜è½¬åŒ–ä¸ºå›¾ç€è‰²çš„é—®é¢˜æ¥æ±‚è§£ï¼Œå°†æ‰€æœ‰çš„ç‰¹å¾è§†ä¸ºå›¾çš„å„ä¸ªé¡¶ç‚¹ï¼Œå°†ä¸æ˜¯ç›¸äº’ç‹¬ç«‹çš„ç‰¹å¾ç”¨ä¸€æ¡è¾¹è¿æ¥èµ·æ¥ï¼Œè¾¹çš„æƒé‡å°±æ˜¯ä¸¤ä¸ªç›¸è¿æ¥çš„ç‰¹å¾çš„æ€»å†²çªå€¼ï¼Œè¿™æ ·éœ€è¦ç»‘å®šçš„ç‰¹å¾å°±æ˜¯åœ¨å›¾ç€è‰²é—®é¢˜ä¸­è¦æ¶‚ä¸ŠåŒä¸€ç§é¢œè‰²çš„é‚£äº›ç‚¹ï¼ˆç‰¹å¾ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°é€šå¸¸æœ‰å¾ˆå¤šç‰¹å¾ï¼Œå°½ç®¡ä¸æ˜¯100ï¼…ç›¸äº’æ’æ–¥ï¼Œä½†ä¹Ÿå¾ˆå°‘åŒæ—¶å–éé›¶å€¼ã€‚ å¦‚æœæˆ‘ä»¬çš„ç®—æ³•å¯ä»¥å…è®¸ä¸€å°éƒ¨åˆ†çš„å†²çªï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´å°‘çš„ç‰¹å¾åŒ…ï¼Œè¿›ä¸€æ­¥æé«˜è®¡ç®—æ•ˆç‡ã€‚ç»è¿‡ç®€å•çš„è®¡ç®—ï¼Œéšæœºæ±¡æŸ“å°éƒ¨åˆ†ç‰¹å¾å€¼å°†å½±å“ç²¾åº¦æœ€å¤š $O\left([(1-\gamma) n]^{-2 / 3}\right)$ï¼Œ $\gamma$ æ˜¯æ¯ä¸ªç»‘å®šä¸­çš„æœ€å¤§å†²çªæ¯”ç‡ï¼Œå½“å…¶ç›¸å¯¹è¾ƒå°æ—¶ï¼Œèƒ½å¤Ÿå®Œæˆç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚å…·ä½“æ­¥éª¤å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š

1. æ„é€ ä¸€ä¸ªåŠ æƒæ— å‘å›¾ï¼Œé¡¶ç‚¹æ˜¯ç‰¹å¾ï¼Œè¾¹æœ‰æƒé‡ï¼Œå…¶æƒé‡ä¸ä¸¤ä¸ªç‰¹å¾é—´å†²çªç›¸å…³ï¼›
2. æ ¹æ®èŠ‚ç‚¹çš„åº¦è¿›è¡Œé™åºæ’åºï¼Œåº¦è¶Šå¤§ï¼Œä¸å…¶å®ƒç‰¹å¾çš„å†²çªè¶Šå¤§ï¼›
3. éå†æ¯ä¸ªç‰¹å¾ï¼Œå°†å®ƒåˆ†é…ç»™ç°æœ‰ç‰¹å¾åŒ…ï¼Œæˆ–è€…æ–°å»ºä¸€ä¸ªç‰¹å¾åŒ…ï¼Œä½¿å¾—æ€»ä½“å†²çªæœ€å°ã€‚

 ![img](/assets/Gradient%20boosting.assets/v2-1b2636a948ece17fae81be7f400fedfc_720w.webp)

ç®—æ³•3çš„æ—¶é—´å¤æ‚åº¦æ˜¯ $O(\#feature^2)$ ï¼Œè®­ç»ƒä¹‹å‰åªå¤„ç†ä¸€æ¬¡ï¼Œå…¶æ—¶é—´å¤æ‚åº¦åœ¨ç‰¹å¾ä¸æ˜¯ç‰¹åˆ«å¤šçš„æƒ…å†µä¸‹æ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†éš¾ä»¥åº”å¯¹ç™¾ä¸‡ç»´åº¦çš„ç‰¹å¾ã€‚ä¸ºäº†ç»§ç»­æé«˜æ•ˆç‡ï¼ŒLightGBMæå‡ºäº†ä¸€ç§æ›´åŠ é«˜æ•ˆçš„æ— å›¾çš„æ’åºç­–ç•¥ï¼šå°†ç‰¹å¾æŒ‰ç…§éé›¶å€¼ä¸ªæ•°æ’åºï¼Œè¿™å’Œä½¿ç”¨å›¾èŠ‚ç‚¹çš„åº¦æ’åºç›¸ä¼¼ï¼Œå› ä¸ºæ›´å¤šçš„éé›¶å€¼é€šå¸¸ä¼šå¯¼è‡´å†²çªï¼Œæ–°ç®—æ³•åœ¨ç®—æ³•3åŸºç¡€ä¸Šæ”¹å˜äº†æ’åºç­–ç•¥ã€‚

#### **è§£å†³æ€ä¹ˆæŠŠç‰¹å¾ç»‘ä¸ºä¸€æ†**

ç‰¹å¾åˆå¹¶ç®—æ³•ï¼Œå…¶å…³é”®åœ¨äºåŸå§‹ç‰¹å¾èƒ½ä»åˆå¹¶çš„ç‰¹å¾ä¸­åˆ†ç¦»å‡ºæ¥ã€‚ç»‘å®šå‡ ä¸ªç‰¹å¾åœ¨åŒä¸€ä¸ªbundleé‡Œéœ€è¦ä¿è¯ç»‘å®šå‰çš„åŸå§‹ç‰¹å¾çš„å€¼å¯ä»¥åœ¨bundleä¸­è¯†åˆ«ï¼Œè€ƒè™‘åˆ°histogram-basedç®—æ³•å°†è¿ç»­çš„å€¼ä¿å­˜ä¸ºç¦»æ•£çš„binsï¼Œæˆ‘ä»¬å¯ä»¥ä½¿å¾—ä¸åŒç‰¹å¾çš„å€¼åˆ†åˆ°bundleä¸­çš„ä¸åŒbinï¼ˆç®±å­ï¼‰ä¸­ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨ç‰¹å¾å€¼ä¸­åŠ ä¸€ä¸ªåç½®å¸¸é‡æ¥è§£å†³ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬åœ¨bundleä¸­ç»‘å®šäº†ä¸¤ä¸ªç‰¹å¾Aå’ŒBï¼ŒAç‰¹å¾çš„åŸå§‹å–å€¼ä¸ºåŒºé—´[0,10)ï¼ŒBç‰¹å¾çš„åŸå§‹å–å€¼ä¸ºåŒºé—´[0,20ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨Bç‰¹å¾çš„å–å€¼ä¸ŠåŠ ä¸€ä¸ªåç½®å¸¸é‡10ï¼Œå°†å…¶å–å€¼èŒƒå›´å˜ä¸º[10,30ï¼‰ï¼Œç»‘å®šåçš„ç‰¹å¾å–å€¼èŒƒå›´ä¸º [0, 30ï¼‰ï¼Œè¿™æ ·å°±å¯ä»¥æ”¾å¿ƒçš„èåˆç‰¹å¾Aå’ŒBäº†ã€‚å…·ä½“çš„ç‰¹å¾åˆå¹¶ç®—æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š

![img](/assets/Gradient%20boosting.assets/v2-cb95a14f542d9cb791df65b63e3f7fdb_720w.webp)

## 